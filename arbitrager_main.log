2022-12-16 18:37:19,043 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:26)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:12)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:12)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-16 18:37:19,058 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-16 18:37:20,919 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-16 18:37:21,711 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-16 18:37:21,711 INFO  [main] MainProducer$ - PG user: m_user
2022-12-16 18:37:21,711 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-16 18:37:24,680 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-16 18:37:24,776 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.6.0
2022-12-16 18:37:24,776 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 62abe01bee039651
2022-12-16 18:37:24,776 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671205044774
2022-12-16 18:37:25,078 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-16 18:41:01,701 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:26)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:12)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:12)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-16 18:41:01,716 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-16 18:41:03,437 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-16 18:41:04,167 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-16 18:41:04,167 INFO  [main] MainProducer$ - PG user: m_user
2022-12-16 18:41:04,167 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-16 18:41:07,006 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-16 18:41:07,079 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.6.0
2022-12-16 18:41:07,080 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 62abe01bee039651
2022-12-16 18:41:07,080 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671205267077
2022-12-16 18:41:07,357 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-18 14:54:09,119 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:33)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-18 14:54:09,135 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 14:54:11,168 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-18 14:54:11,959 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-18 14:54:11,959 INFO  [main] MainProducer$ - PG user: m_user
2022-12-18 14:54:11,959 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-18 14:54:14,872 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-18 14:54:14,938 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.6.0
2022-12-18 14:54:14,938 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 62abe01bee039651
2022-12-18 14:54:14,938 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671364454936
2022-12-18 14:54:15,221 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-18 14:56:03,166 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 14:56:03,244 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 14:56:05,617 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 14:56:05,617 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 14:56:05,617 INFO  [main] MainDataLoader$ - request secs: List(AMZN)
2022-12-18 14:56:05,617 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 14:56:05,617 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 14:56:09,225 INFO  [main] MainDataLoader$ - try to write 20 rows in m_data.t_source_marketdata
2022-12-18 14:56:09,689 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:09,690 INFO  [main] MainDataLoader$ - Gurrent page set to 1
2022-12-18 14:56:10,010 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:10,096 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:10,096 INFO  [main] MainDataLoader$ - Gurrent page set to 2
2022-12-18 14:56:10,394 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:10,465 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:10,465 INFO  [main] MainDataLoader$ - Gurrent page set to 3
2022-12-18 14:56:10,758 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:10,829 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:10,829 INFO  [main] MainDataLoader$ - Gurrent page set to 4
2022-12-18 14:56:11,102 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:11,164 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:11,164 INFO  [main] MainDataLoader$ - Gurrent page set to 5
2022-12-18 14:56:11,417 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:11,470 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:11,470 INFO  [main] MainDataLoader$ - Gurrent page set to 6
2022-12-18 14:56:11,744 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:11,803 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:11,803 INFO  [main] MainDataLoader$ - Gurrent page set to 7
2022-12-18 14:56:12,056 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:12,106 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:12,106 INFO  [main] MainDataLoader$ - Gurrent page set to 8
2022-12-18 14:56:12,352 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:12,399 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:12,399 INFO  [main] MainDataLoader$ - Gurrent page set to 9
2022-12-18 14:56:12,667 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:12,718 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:12,718 INFO  [main] MainDataLoader$ - Gurrent page set to 10
2022-12-18 14:56:12,966 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:13,017 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:13,017 INFO  [main] MainDataLoader$ - Gurrent page set to 11
2022-12-18 14:56:13,277 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:13,330 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:13,331 INFO  [main] MainDataLoader$ - Gurrent page set to 12
2022-12-18 14:56:13,594 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:13,656 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:13,656 INFO  [main] MainDataLoader$ - Gurrent page set to 13
2022-12-18 14:56:13,902 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:13,951 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:13,952 INFO  [main] MainDataLoader$ - Gurrent page set to 14
2022-12-18 14:56:14,187 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:14,230 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:14,231 INFO  [main] MainDataLoader$ - Gurrent page set to 15
2022-12-18 14:56:14,483 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:14,540 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:14,541 INFO  [main] MainDataLoader$ - Gurrent page set to 16
2022-12-18 14:56:14,789 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:14,843 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:14,843 INFO  [main] MainDataLoader$ - Gurrent page set to 17
2022-12-18 14:56:15,086 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:15,132 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:15,132 INFO  [main] MainDataLoader$ - Gurrent page set to 18
2022-12-18 14:56:15,371 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:15,415 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:15,415 INFO  [main] MainDataLoader$ - Gurrent page set to 19
2022-12-18 14:56:15,650 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:15,715 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:15,715 INFO  [main] MainDataLoader$ - Gurrent page set to 20
2022-12-18 14:56:15,949 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:15,996 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:15,996 INFO  [main] MainDataLoader$ - Gurrent page set to 21
2022-12-18 14:56:16,228 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:16,279 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:16,279 INFO  [main] MainDataLoader$ - Gurrent page set to 22
2022-12-18 14:56:16,540 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:16,603 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:16,603 INFO  [main] MainDataLoader$ - Gurrent page set to 23
2022-12-18 14:56:16,849 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:16,910 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:16,910 INFO  [main] MainDataLoader$ - Gurrent page set to 24
2022-12-18 14:56:17,144 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:17,188 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:17,188 INFO  [main] MainDataLoader$ - Gurrent page set to 25
2022-12-18 14:56:17,418 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:17,467 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:17,467 INFO  [main] MainDataLoader$ - Gurrent page set to 26
2022-12-18 14:56:17,709 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:17,753 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:17,753 INFO  [main] MainDataLoader$ - Gurrent page set to 27
2022-12-18 14:56:17,984 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:18,046 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:18,046 INFO  [main] MainDataLoader$ - Gurrent page set to 28
2022-12-18 14:56:18,284 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:18,332 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:18,333 INFO  [main] MainDataLoader$ - Gurrent page set to 29
2022-12-18 14:56:18,561 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:18,606 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:18,606 INFO  [main] MainDataLoader$ - Gurrent page set to 30
2022-12-18 14:56:18,865 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:18,907 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:18,907 INFO  [main] MainDataLoader$ - Gurrent page set to 31
2022-12-18 14:56:19,143 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:19,189 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:19,189 INFO  [main] MainDataLoader$ - Gurrent page set to 32
2022-12-18 14:56:19,420 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:19,464 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:19,464 INFO  [main] MainDataLoader$ - Gurrent page set to 33
2022-12-18 14:56:19,693 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:19,734 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:19,735 INFO  [main] MainDataLoader$ - Gurrent page set to 34
2022-12-18 14:56:19,959 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:20,002 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:20,002 INFO  [main] MainDataLoader$ - Gurrent page set to 35
2022-12-18 14:56:20,245 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:20,295 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:20,296 INFO  [main] MainDataLoader$ - Gurrent page set to 36
2022-12-18 14:56:20,548 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:20,589 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:20,589 INFO  [main] MainDataLoader$ - Gurrent page set to 37
2022-12-18 14:56:20,822 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:20,864 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:20,864 INFO  [main] MainDataLoader$ - Gurrent page set to 38
2022-12-18 14:56:21,094 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:21,133 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:21,134 INFO  [main] MainDataLoader$ - Gurrent page set to 39
2022-12-18 14:56:21,379 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:21,421 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:21,421 INFO  [main] MainDataLoader$ - Gurrent page set to 40
2022-12-18 14:56:21,657 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:21,700 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:21,700 INFO  [main] MainDataLoader$ - Gurrent page set to 41
2022-12-18 14:56:21,925 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:21,967 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:21,967 INFO  [main] MainDataLoader$ - Gurrent page set to 42
2022-12-18 14:56:22,206 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:22,249 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:22,249 INFO  [main] MainDataLoader$ - Gurrent page set to 43
2022-12-18 14:56:22,483 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:22,525 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:22,525 INFO  [main] MainDataLoader$ - Gurrent page set to 44
2022-12-18 14:56:22,765 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:22,814 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:22,815 INFO  [main] MainDataLoader$ - Gurrent page set to 45
2022-12-18 14:56:23,051 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:23,092 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:23,092 INFO  [main] MainDataLoader$ - Gurrent page set to 46
2022-12-18 14:56:23,328 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:23,368 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:23,368 INFO  [main] MainDataLoader$ - Gurrent page set to 47
2022-12-18 14:56:23,597 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:23,640 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:23,640 INFO  [main] MainDataLoader$ - Gurrent page set to 48
2022-12-18 14:56:23,870 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:23,912 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:23,912 INFO  [main] MainDataLoader$ - Gurrent page set to 49
2022-12-18 14:56:24,146 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:24,190 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:24,190 INFO  [main] MainDataLoader$ - Gurrent page set to 50
2022-12-18 14:56:24,434 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:24,477 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:24,477 INFO  [main] MainDataLoader$ - Gurrent page set to 51
2022-12-18 14:56:24,715 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:24,756 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:24,756 INFO  [main] MainDataLoader$ - Gurrent page set to 52
2022-12-18 14:56:24,986 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:25,032 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:25,032 INFO  [main] MainDataLoader$ - Gurrent page set to 53
2022-12-18 14:56:25,262 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:25,310 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:25,310 INFO  [main] MainDataLoader$ - Gurrent page set to 54
2022-12-18 14:56:25,539 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:25,587 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:25,587 INFO  [main] MainDataLoader$ - Gurrent page set to 55
2022-12-18 14:56:25,829 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:25,870 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:25,870 INFO  [main] MainDataLoader$ - Gurrent page set to 56
2022-12-18 14:56:26,118 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:26,163 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:26,163 INFO  [main] MainDataLoader$ - Gurrent page set to 57
2022-12-18 14:56:26,387 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:26,428 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:26,428 INFO  [main] MainDataLoader$ - Gurrent page set to 58
2022-12-18 14:56:26,660 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:26,703 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:26,704 INFO  [main] MainDataLoader$ - Gurrent page set to 59
2022-12-18 14:56:26,941 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:26,982 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:26,982 INFO  [main] MainDataLoader$ - Gurrent page set to 60
2022-12-18 14:56:27,226 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:27,267 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:27,268 INFO  [main] MainDataLoader$ - Gurrent page set to 61
2022-12-18 14:56:27,708 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:27,749 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:27,749 INFO  [main] MainDataLoader$ - Gurrent page set to 62
2022-12-18 14:56:27,988 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:28,031 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:28,032 INFO  [main] MainDataLoader$ - Gurrent page set to 63
2022-12-18 14:56:28,291 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:28,336 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:28,336 INFO  [main] MainDataLoader$ - Gurrent page set to 64
2022-12-18 14:56:28,567 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 14:56:28,612 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 14:56:28,612 INFO  [main] MainDataLoader$ - Gurrent page set to 65
2022-12-18 14:56:28,854 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:14:19,206 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:14:19,283 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:14:21,520 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:14:21,521 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:14:21,521 INFO  [main] MainDataLoader$ - request secs: List(GOOG)
2022-12-18 15:14:21,521 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:14:21,521 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:15:04,546 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:15:04,621 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:15:06,837 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:15:06,837 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:15:06,837 INFO  [main] MainDataLoader$ - request secs: List(GOOG)
2022-12-18 15:15:06,838 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:15:06,838 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:16:59,358 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:16:59,447 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:17:01,750 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:17:01,750 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:17:01,750 INFO  [main] MainDataLoader$ - request secs: List(GOOG)
2022-12-18 15:17:01,750 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:17:01,750 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:19:00,258 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:19:00,346 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:19:02,627 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:19:02,627 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:19:02,627 INFO  [main] MainDataLoader$ - request secs: List(GOOG)
2022-12-18 15:19:02,627 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:19:02,627 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:19:17,602 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:19:17,679 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:19:19,862 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:19:19,862 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:19:19,862 INFO  [main] MainDataLoader$ - request secs: List(GOOG)
2022-12-18 15:19:19,862 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:19:19,862 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:19:58,509 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:19:58,590 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:20:00,851 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:20:00,851 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:20:00,851 INFO  [main] MainDataLoader$ - request secs: List(BABA)
2022-12-18 15:20:00,851 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:20:00,851 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:20:04,063 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:04,366 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:04,367 INFO  [main] MainDataLoader$ - Gurrent page set to 1 (of 1). Offset - 1000. TotalRowCount - 0.
2022-12-18 15:20:04,691 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:04,774 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:04,775 INFO  [main] MainDataLoader$ - Gurrent page set to 2 (of 1). Offset - 2000. TotalRowCount - 0.
2022-12-18 15:20:05,061 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:05,131 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:05,131 INFO  [main] MainDataLoader$ - Gurrent page set to 3 (of 1). Offset - 3000. TotalRowCount - 0.
2022-12-18 15:20:05,402 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:05,484 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:05,484 INFO  [main] MainDataLoader$ - Gurrent page set to 4 (of 1). Offset - 4000. TotalRowCount - 0.
2022-12-18 15:20:05,762 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:05,816 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:05,816 INFO  [main] MainDataLoader$ - Gurrent page set to 5 (of 1). Offset - 5000. TotalRowCount - 0.
2022-12-18 15:20:06,058 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:06,111 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:06,111 INFO  [main] MainDataLoader$ - Gurrent page set to 6 (of 1). Offset - 6000. TotalRowCount - 0.
2022-12-18 15:20:06,349 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:06,406 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:06,407 INFO  [main] MainDataLoader$ - Gurrent page set to 7 (of 1). Offset - 7000. TotalRowCount - 0.
2022-12-18 15:20:06,644 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:20:06,697 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:20:06,697 INFO  [main] MainDataLoader$ - Gurrent page set to 8 (of 1). Offset - 8000. TotalRowCount - 0.
2022-12-18 15:21:31,184 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:21:31,268 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:21:33,465 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@4ef18604.data_type
2022-12-18 15:21:33,465 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@4ef18604.first_dt to MainDataLoader$InitList@4ef18604.last_dt
2022-12-18 15:21:33,465 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:21:33,465 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:21:33,465 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@4ef18604.adapter
2022-12-18 15:21:33,876 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:33,876 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:36,945 INFO  [main] MainDataLoader$ - try to write 20 rows in m_data.t_source_marketdata
2022-12-18 15:21:37,366 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:37,366 INFO  [main] MainDataLoader$ - Gurrent page set to 1 (of 0). Offset - 1000. TotalRowCount - 20.
2022-12-18 15:21:37,569 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:37,570 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:37,671 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:37,745 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:37,745 INFO  [main] MainDataLoader$ - Gurrent page set to 2 (of 0). Offset - 2000. TotalRowCount - 20.
2022-12-18 15:21:37,942 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:37,942 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:38,027 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:38,102 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:38,102 INFO  [main] MainDataLoader$ - Gurrent page set to 3 (of 0). Offset - 3000. TotalRowCount - 20.
2022-12-18 15:21:38,289 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:38,289 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:38,374 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:38,438 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:38,439 INFO  [main] MainDataLoader$ - Gurrent page set to 4 (of 0). Offset - 4000. TotalRowCount - 20.
2022-12-18 15:21:38,634 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:38,634 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:38,704 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:38,779 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:38,779 INFO  [main] MainDataLoader$ - Gurrent page set to 5 (of 0). Offset - 5000. TotalRowCount - 20.
2022-12-18 15:21:38,965 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:38,966 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:39,022 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:39,079 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:39,079 INFO  [main] MainDataLoader$ - Gurrent page set to 6 (of 0). Offset - 6000. TotalRowCount - 20.
2022-12-18 15:21:39,275 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:39,275 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:39,334 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:39,393 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:39,394 INFO  [main] MainDataLoader$ - Gurrent page set to 7 (of 0). Offset - 7000. TotalRowCount - 20.
2022-12-18 15:21:39,585 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 20
2022-12-18 15:21:39,585 INFO  [main] MainDataLoader$ - Page amount set to 0 rows
2022-12-18 15:21:39,637 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:21:39,686 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:21:39,686 INFO  [main] MainDataLoader$ - Gurrent page set to 8 (of 0). Offset - 8000. TotalRowCount - 20.
2022-12-18 15:29:52,184 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:29:52,277 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:29:54,555 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:29:54,555 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:29:54,556 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:29:54,556 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:29:54,556 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:29:55,033 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 42
2022-12-18 15:29:55,033 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2022-12-18 15:29:58,087 INFO  [main] MainDataLoader$ - try to write 42 rows in m_data.t_source_marketdata
2022-12-18 15:29:58,507 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:29:58,507 INFO  [main] MainDataLoader$ - Gurrent page set to 1 (of 1). Offset - 1000. TotalRowCount - 42.
2022-12-18 15:29:58,805 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:29:58,883 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:29:58,883 INFO  [main] MainDataLoader$ - Gurrent page set to 2 (of 1). Offset - 2000. TotalRowCount - 42.
2022-12-18 15:32:38,068 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:32:38,147 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:32:40,381 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:32:40,381 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:32:40,381 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:32:40,381 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:32:40,381 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:33:54,117 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:33:54,197 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:33:56,419 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:33:56,420 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:33:56,420 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:33:56,420 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:33:56,420 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:33:56,887 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 41
2022-12-18 15:33:56,887 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2022-12-18 15:33:59,947 INFO  [main] MainDataLoader$ - try to write 41 rows in m_data.t_source_marketdata
2022-12-18 15:34:00,405 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:34:00,405 INFO  [main] MainDataLoader$ - Current page set to 1 (of 1). Offset - 1000. TotalRowCount - 41.
2022-12-18 15:34:00,758 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:34:00,868 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:34:00,869 INFO  [main] MainDataLoader$ - Current page set to 2 (of 1). Offset - 2000. TotalRowCount - 41.
2022-12-18 15:37:06,882 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:37:06,964 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:37:09,209 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:37:09,209 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:37:09,209 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:37:09,209 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:37:09,209 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:37:09,686 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 18
2022-12-18 15:37:09,686 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2022-12-18 15:37:12,752 INFO  [main] MainDataLoader$ - try to write 18 rows in m_data.t_source_marketdata
2022-12-18 15:37:13,213 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:37:13,213 INFO  [main] MainDataLoader$ - Current page set to 1 (of 1). Offset - 1000. TotalRowCount - 18.
2022-12-18 15:37:13,544 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:37:13,647 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:37:13,647 INFO  [main] MainDataLoader$ - Current page set to 2 (of 1). Offset - 2000. TotalRowCount - 18.
2022-12-18 15:40:08,475 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:40:08,548 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:40:10,812 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@372f0a99.data_type
2022-12-18 15:40:10,812 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@372f0a99.first_dt to MainDataLoader$InitList@372f0a99.last_dt
2022-12-18 15:40:10,812 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:40:10,812 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:40:10,812 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@372f0a99.adapter
2022-12-18 15:40:11,290 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 0
2022-12-18 15:40:11,290 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2022-12-18 15:40:14,106 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:14,404 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:40:14,404 INFO  [main] MainDataLoader$ - Current page set to 1 (of 1). Offset - 1000. TotalRowCount - 0.
2022-12-18 15:40:14,999 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:15,096 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:40:15,389 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:15,466 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:40:15,758 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:15,833 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:40:17,283 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:17,343 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:40:17,606 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2022-12-18 15:40:17,660 INFO  [main] MainDataLoader$ - pg load complete
2022-12-18 15:42:31,551 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3482)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3477)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3319)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:479)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2022-12-18 15:42:31,636 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-18 15:42:33,847 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@4ef18604.data_type
2022-12-18 15:42:33,847 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@4ef18604.first_dt to MainDataLoader$InitList@4ef18604.last_dt
2022-12-18 15:42:33,847 INFO  [main] MainDataLoader$ - request secs: List(FB)
2022-12-18 15:42:33,847 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2022-12-18 15:42:33,847 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@4ef18604.adapter
2022-12-20 20:33:18,064 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:33)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-20 20:33:18,078 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-20 20:33:19,839 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-20 20:33:20,654 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-20 20:33:20,655 INFO  [main] MainProducer$ - PG user: m_user
2022-12-20 20:33:20,655 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-20 20:33:23,651 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-20 20:33:23,722 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 2.6.0
2022-12-20 20:33:23,722 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: 62abe01bee039651
2022-12-20 20:33:23,722 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671557603720
2022-12-20 20:33:24,010 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-23 22:15:36,984 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-23 22:15:37,005 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-23 22:19:38,434 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-23 22:19:38,451 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-23 22:19:38,995 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:36)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:19)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:19)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 22:19:39,008 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 22:19:40,732 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 22:19:41,504 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 22:19:41,504 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 22:19:41,504 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 22:30:47,549 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-23 22:30:47,567 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-23 22:30:48,020 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:36)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:19)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:19)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 22:30:48,029 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 22:30:49,473 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 22:30:49,995 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 22:30:49,995 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 22:30:49,995 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 22:46:29,174 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-23 22:46:29,189 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-23 22:46:29,654 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:36)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:19)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:19)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 22:46:29,664 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 22:46:31,134 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 22:46:31,633 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 22:46:31,633 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 22:46:31,633 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 22:47:16,377 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:36)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:19)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:19)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 22:47:16,390 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 22:47:17,907 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 22:47:18,393 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 22:47:18,393 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 22:47:18,393 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 22:48:09,171 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 22:48:09,184 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 22:48:10,719 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 22:48:11,222 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 22:48:11,222 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 22:48:11,222 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 23:25:50,875 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 23:25:50,888 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 23:25:52,463 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 23:25:52,946 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 23:25:52,946 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 23:25:52,947 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 23:52:28,516 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 23:52:28,529 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-23 23:52:30,344 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-23 23:52:31,076 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-23 23:52:31,076 INFO  [main] MainProducer$ - PG user: m_user
2022-12-23 23:52:31,076 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-23 23:57:36,531 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-23 23:57:36,543 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 00:08:09,252 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 00:08:09,265 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 00:08:10,815 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 00:08:11,295 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 00:08:11,295 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 00:08:11,295 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 16:25:30,005 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:27)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:13)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:13)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 16:25:30,019 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 16:25:31,865 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 16:25:32,629 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 16:25:32,629 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 16:25:32,629 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 16:25:35,958 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 16:25:36,032 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 16:25:36,033 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 16:25:36,033 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671888336030
2022-12-24 16:25:36,389 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-24 16:37:03,318 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 16:37:03,329 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 16:37:04,864 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 16:37:04,870 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 16:37:04,885 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 16:41:20,160 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 16:41:20,171 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 16:41:21,697 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 16:41:21,702 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 16:41:21,713 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 16:44:19,849 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 16:44:19,861 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 16:44:21,365 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 16:44:21,369 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 16:44:21,381 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 16:59:40,959 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 16:59:40,973 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 16:59:42,826 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 16:59:42,831 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 16:59:42,843 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:00:05,940 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 17:00:05,951 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:00:07,474 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:00:07,477 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:00:07,487 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:03:16,347 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 17:03:16,359 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:03:17,907 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:03:17,913 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:03:17,924 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:03:18,515 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:03:18,515 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:03:18,515 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:04:58,689 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 17:04:58,701 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:05:00,233 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:05:00,236 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:05:00,247 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:05:00,827 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:05:00,827 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:05:00,827 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:05:03,188 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:05:03,244 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:05:03,244 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:05:03,244 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671890703242
2022-12-24 17:05:03,294 ERROR [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NoSuchMethodError: org.apache.spark.sql.internal.SQLConf$.AVRO_REBASE_MODE_IN_WRITE()Lorg/apache/spark/internal/config/ConfigEntry;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:05:03,320 ERROR [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2022-12-24 17:28:36,526 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 17:28:36,546 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:28:38,117 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:28:38,123 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:28:38,139 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:28:38,850 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:28:38,850 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:28:38,850 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:29:35,571 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:78)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1665)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:316)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:304)
	at org.apache.hadoop.security.UserGroupInformation.doSubjectLogin(UserGroupInformation.java:1828)
	at org.apache.hadoop.security.UserGroupInformation.createLoginUser(UserGroupInformation.java:710)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:660)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:571)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2487)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2487)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2672)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:945)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:939)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 29 common frames omitted
2022-12-24 17:29:35,586 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:29:37,167 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:29:37,173 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:29:37,186 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:29:37,831 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:29:37,831 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:29:37,831 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:29:40,263 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:29:40,323 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:29:40,324 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:29:40,324 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671892180322
2022-12-24 17:29:40,363 ERROR [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NoSuchMethodError: org.apache.spark.sql.internal.SQLConf$.AVRO_REBASE_MODE_IN_WRITE()Lorg/apache/spark/internal/config/ConfigEntry;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:755)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2236)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:29:40,386 ERROR [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2022-12-24 17:33:49,328 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:33:49,341 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:33:50,924 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:33:50,929 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:33:50,941 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:33:51,780 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:33:51,780 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:33:51,780 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:33:54,259 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:33:54,309 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:33:54,310 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:33:54,310 INFO  [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671892434307
2022-12-24 17:33:54,347 ERROR [Executor task launch worker for task 0.0 in stage 0.0 (TID 0)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:33:54,368 ERROR [task-result-getter-0] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 0.0 failed 1 times; aborting job
2022-12-24 17:41:03,740 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:41:03,750 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:41:05,349 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:41:05,354 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:41:05,366 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:41:06,227 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:41:06,227 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:41:06,227 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:41:08,961 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:41:09,019 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:41:09,019 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:41:09,019 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671892869016
2022-12-24 17:41:09,050 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:41:09,076 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-24 17:48:10,059 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:48:10,073 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:48:11,636 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:48:11,643 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:48:11,655 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:49:17,797 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:49:17,808 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:49:19,295 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:49:19,300 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:49:19,312 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:49:20,147 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:49:20,147 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:49:20,147 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:49:22,887 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:49:22,942 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:49:22,942 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:49:22,942 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671893362940
2022-12-24 17:49:22,974 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:49:22,997 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-24 17:50:35,083 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:50:35,094 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:50:36,610 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:50:36,616 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:50:36,629 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:50:37,465 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:50:37,466 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:50:37,466 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:50:40,228 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:50:40,290 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:50:40,290 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:50:40,290 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671893440288
2022-12-24 17:50:40,324 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:50:40,350 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-24 17:55:56,841 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 17:55:56,854 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 17:55:58,426 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 17:55:58,431 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 17:55:58,442 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 17:55:59,245 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 17:55:59,245 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 17:55:59,245 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 17:56:02,004 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 17:56:02,062 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 17:56:02,062 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 17:56:02,062 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671893762061
2022-12-24 17:56:02,109 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 17:56:02,132 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-24 18:05:51,974 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 18:05:51,990 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 18:05:53,561 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 18:05:53,566 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 18:05:53,579 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 18:05:54,394 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 18:05:54,395 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 18:05:54,395 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 18:05:57,140 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 18:05:57,196 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 18:05:57,196 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 18:05:57,196 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671894357194
2022-12-24 18:05:57,228 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 18:05:57,250 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-24 18:15:59,390 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 18:15:59,402 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 18:16:01,025 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 18:16:01,031 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 18:16:01,045 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 18:16:01,881 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 18:16:01,881 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 18:16:01,881 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 18:18:17,778 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 18:18:17,789 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 18:18:19,309 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 18:18:19,316 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 18:18:19,329 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 18:18:20,135 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 18:18:20,135 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 18:18:20,135 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 18:18:22,947 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 18:18:23,008 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 18:18:23,008 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 18:18:23,008 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671895103006
2022-12-24 18:18:23,303 INFO  [kafka-producer-network-thread | producer-1] org.apache.kafka.clients.Metadata - [Producer clientId=producer-1] Cluster ID: 8tAHpms4TxCp14Y65c9jpw
2022-12-24 18:20:14,505 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-24 18:20:14,518 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-24 18:20:16,022 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-24 18:20:16,027 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-24 18:20:16,040 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-24 18:20:16,883 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-24 18:20:16,883 INFO  [main] MainProducer$ - PG user: m_user
2022-12-24 18:20:16,883 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-24 18:20:19,639 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-24 18:20:19,694 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-24 18:20:19,694 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-24 18:20:19,694 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1671895219692
2022-12-24 18:20:19,725 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-24 18:20:19,749 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 19:03:59,735 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 19:03:59,748 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 19:04:01,397 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 19:04:01,403 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 19:04:01,418 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 19:04:02,532 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 19:04:02,532 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 19:04:02,532 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 19:04:05,347 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 19:04:05,405 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 19:04:05,405 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 19:04:05,405 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672157045404
2022-12-27 19:04:05,438 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 19:04:05,461 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 19:05:36,840 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 19:05:36,853 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 19:05:38,451 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 19:05:38,457 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 19:05:38,471 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 19:05:39,322 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 19:05:39,322 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 19:05:39,322 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 19:05:41,991 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 19:05:42,051 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 19:05:42,051 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 19:05:42,051 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672157142050
2022-12-27 19:05:42,087 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 19:05:42,110 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 19:12:19,508 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:30)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 19:12:19,522 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 19:12:21,088 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 19:12:21,093 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 19:12:21,105 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 19:12:21,954 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 19:12:21,954 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 19:12:21,954 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 19:12:24,645 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 19:12:24,703 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 19:12:24,703 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 19:12:24,703 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672157544701
2022-12-27 19:12:24,740 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 19:12:24,761 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 20:20:26,058 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 20:20:26,070 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 20:20:27,598 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 20:20:27,602 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 20:20:27,614 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 20:20:28,513 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 20:20:28,513 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 20:20:28,513 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 20:20:31,329 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 20:20:31,382 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 20:20:31,382 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 20:20:31,382 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672161631381
2022-12-27 20:20:31,415 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 20:20:31,440 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 20:21:17,808 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 20:21:17,821 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 20:21:19,336 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 20:21:19,341 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 20:21:19,353 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 20:21:20,176 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 20:21:20,176 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 20:21:20,176 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 20:21:23,091 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 20:21:23,147 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 20:21:23,147 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 20:21:23,147 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672161683145
2022-12-27 20:21:23,179 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 20:21:23,205 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-27 20:53:21,655 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-27 20:53:21,670 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-27 20:53:23,533 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-27 20:53:23,541 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-27 20:53:23,561 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-27 20:53:24,710 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-27 20:53:24,710 INFO  [main] MainProducer$ - PG user: m_user
2022-12-27 20:53:24,710 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-27 20:53:27,783 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-27 20:53:27,860 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-27 20:53:27,860 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-27 20:53:27,860 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672163607856
2022-12-27 20:53:27,893 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-27 20:53:27,915 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2022-12-28 18:30:18,260 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2022-12-28 18:30:18,274 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2022-12-28 18:30:20,107 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2022-12-28 18:30:20,114 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2022-12-28 18:30:20,137 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2022-12-28 18:30:21,400 INFO  [main] MainProducer$ - PG url: jdbc:postgresql://localhost:5432/m_db
2022-12-28 18:30:21,401 INFO  [main] MainProducer$ - PG user: m_user
2022-12-28 18:30:21,401 INFO  [main] MainProducer$ - PG pass: m_user
2022-12-28 18:30:24,524 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.clients.producer.ProducerConfig - ProducerConfig values: 
	acks = 1
	batch.size = 16384
	bootstrap.servers = [localhost:9092]
	buffer.memory = 33554432
	client.dns.lookup = use_all_dns_ips
	client.id = producer-1
	compression.type = none
	connections.max.idle.ms = 540000
	delivery.timeout.ms = 120000
	enable.idempotence = false
	interceptor.classes = []
	internal.auto.downgrade.txn.commit = false
	key.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer
	linger.ms = 0
	max.block.ms = 60000
	max.in.flight.requests.per.connection = 5
	max.request.size = 1048576
	metadata.max.age.ms = 300000
	metadata.max.idle.ms = 300000
	metric.reporters = []
	metrics.num.samples = 2
	metrics.recording.level = INFO
	metrics.sample.window.ms = 30000
	partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner
	receive.buffer.bytes = 32768
	reconnect.backoff.max.ms = 1000
	reconnect.backoff.ms = 50
	request.timeout.ms = 30000
	retries = 2147483647
	retry.backoff.ms = 100
	sasl.client.callback.handler.class = null
	sasl.jaas.config = null
	sasl.kerberos.kinit.cmd = /usr/bin/kinit
	sasl.kerberos.min.time.before.relogin = 60000
	sasl.kerberos.service.name = null
	sasl.kerberos.ticket.renew.jitter = 0.05
	sasl.kerberos.ticket.renew.window.factor = 0.8
	sasl.login.callback.handler.class = null
	sasl.login.class = null
	sasl.login.refresh.buffer.seconds = 300
	sasl.login.refresh.min.period.seconds = 60
	sasl.login.refresh.window.factor = 0.8
	sasl.login.refresh.window.jitter = 0.05
	sasl.mechanism = GSSAPI
	security.protocol = PLAINTEXT
	security.providers = null
	send.buffer.bytes = 131072
	socket.connection.setup.timeout.max.ms = 30000
	socket.connection.setup.timeout.ms = 10000
	ssl.cipher.suites = null
	ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	ssl.endpoint.identification.algorithm = https
	ssl.engine.factory.class = null
	ssl.key.password = null
	ssl.keymanager.algorithm = SunX509
	ssl.keystore.certificate.chain = null
	ssl.keystore.key = null
	ssl.keystore.location = null
	ssl.keystore.password = null
	ssl.keystore.type = JKS
	ssl.protocol = TLSv1.3
	ssl.provider = null
	ssl.secure.random.implementation = null
	ssl.trustmanager.algorithm = PKIX
	ssl.truststore.certificates = null
	ssl.truststore.location = null
	ssl.truststore.password = null
	ssl.truststore.type = JKS
	transaction.timeout.ms = 60000
	transactional.id = null
	value.serializer = class org.apache.kafka.common.serialization.ByteArraySerializer

2022-12-28 18:30:24,596 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka version: 6.2.1-ccs
2022-12-28 18:30:24,596 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka commitId: fa4bec046a2df3a6
2022-12-28 18:30:24,596 INFO  [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.kafka.common.utils.AppInfoParser - Kafka startTimeMs: 1672241424592
2022-12-28 18:30:24,633 ERROR [Executor task launch worker for task 0.0 in stage 1.0 (TID 1)] org.apache.spark.executor.Executor - Exception in task 0.0 in stage 1.0 (TID 1)
java.lang.NoSuchMethodError: org.apache.spark.sql.execution.datasources.DataSourceUtils$.createDateRebaseFuncInWrite(Lscala/Enumeration$Value;Ljava/lang/String;)Lscala/Function1;
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:64)
	at org.apache.spark.sql.avro.AvroSerializer.<init>(AvroSerializer.scala:56)
	at org.apache.spark.sql.avro.AbrisAvroSerializer.<init>(AbrisAvroSerializer.scala:29)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer$lzycompute(CatalystDataToAvro.scala:41)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.serializer(CatalystDataToAvro.scala:40)
	at za.co.absa.abris.avro.sql.CatalystDataToAvro.nullSafeEval(CatalystDataToAvro.scala:58)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)
	at org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)
	at org.apache.spark.sql.execution.WholeStageCodegenExec$$anon$1.hasNext(WholeStageCodegenExec.scala:759)
	at org.apache.spark.sql.kafka010.KafkaWriteTask.execute(KafkaWriteTask.scala:51)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$2(KafkaWriter.scala:72)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1(KafkaWriter.scala:73)
	at org.apache.spark.sql.kafka010.KafkaWriter$.$anonfun$write$1$adapted(KafkaWriter.scala:70)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2(RDD.scala:1020)
	at org.apache.spark.rdd.RDD.$anonfun$foreachPartition$2$adapted(RDD.scala:1020)
	at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2254)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)
	at org.apache.spark.scheduler.Task.run(Task.scala:131)
	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:506)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1462)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:509)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
2022-12-28 18:30:24,656 ERROR [task-result-getter-1] org.apache.spark.scheduler.TaskSetManager - Task 0 in stage 1.0 failed 1 times; aborting job
2023-01-12 03:24:54,372 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2023-01-12 03:24:54,388 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-01-12 03:24:56,086 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2023-01-12 03:24:56,091 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2023-01-12 03:24:56,106 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2023-01-12 03:25:29,182 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2023-01-12 03:25:29,196 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-01-12 03:25:30,598 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2023-01-12 03:25:30,605 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2023-01-12 03:25:30,617 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2023-01-18 18:39:29,290 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2023-01-18 18:39:29,303 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-01-18 18:39:31,137 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2023-01-18 18:39:31,142 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2023-01-18 18:39:31,160 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2023-02-04 13:18:02,087 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2023-02-04 13:18:02,102 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-02-04 13:18:03,840 INFO  [main] MainProducer$ - start producer. Exchange-XNAS
2023-02-04 13:18:03,846 INFO  [main] za.co.absa.abris.avro.read.confluent.SchemaManagerFactory - Configuring new Schema Registry client of type ConfluentRegistryClient
2023-02-04 13:18:03,865 INFO  [main] io.confluent.kafka.serializers.KafkaAvroDeserializerConfig - KafkaAvroDeserializerConfig values: 
	auto.register.schemas = true
	avro.reflection.allow.null = false
	avro.use.logical.type.converters = false
	basic.auth.credentials.source = URL
	basic.auth.user.info = [hidden]
	bearer.auth.credentials.source = STATIC_TOKEN
	bearer.auth.token = [hidden]
	key.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy
	latest.compatibility.strict = true
	max.schemas.per.subject = 1000
	proxy.host = 
	proxy.port = -1
	schema.reflection = false
	schema.registry.basic.auth.user.info = [hidden]
	schema.registry.ssl.cipher.suites = null
	schema.registry.ssl.enabled.protocols = [TLSv1.2, TLSv1.3]
	schema.registry.ssl.endpoint.identification.algorithm = https
	schema.registry.ssl.engine.factory.class = null
	schema.registry.ssl.key.password = null
	schema.registry.ssl.keymanager.algorithm = SunX509
	schema.registry.ssl.keystore.certificate.chain = null
	schema.registry.ssl.keystore.key = null
	schema.registry.ssl.keystore.location = null
	schema.registry.ssl.keystore.password = null
	schema.registry.ssl.keystore.type = JKS
	schema.registry.ssl.protocol = TLSv1.3
	schema.registry.ssl.provider = null
	schema.registry.ssl.secure.random.implementation = null
	schema.registry.ssl.trustmanager.algorithm = PKIX
	schema.registry.ssl.truststore.certificates = null
	schema.registry.ssl.truststore.location = null
	schema.registry.ssl.truststore.password = null
	schema.registry.ssl.truststore.type = JKS
	schema.registry.url = [http://localhost:8081]
	specific.avro.reader = false
	use.latest.version = false
	value.subject.name.strategy = class io.confluent.kafka.serializers.subject.TopicNameStrategy

2023-04-06 00:58:25,787 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at MainProducer$.delayedEndpoint$MainProducer$1(MainProducer.scala:29)
	at MainProducer$delayedInit$body.apply(MainProducer.scala:16)
	at scala.Function0.apply$mcV$sp(Function0.scala:39)
	at scala.Function0.apply$mcV$sp$(Function0.scala:39)
	at scala.runtime.AbstractFunction0.apply$mcV$sp(AbstractFunction0.scala:17)
	at scala.App.$anonfun$main$1$adapted(App.scala:80)
	at scala.collection.immutable.List.foreach(List.scala:431)
	at scala.App.main(App.scala:80)
	at scala.App.main$(App.scala:78)
	at MainProducer$.main(MainProducer.scala:16)
	at MainProducer.main(MainProducer.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 26 common frames omitted
2023-04-06 00:58:25,802 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-04-06 01:10:44,638 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3741)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2023-04-06 01:10:44,707 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-04-06 01:10:47,147 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@7a606260.data_type
2023-04-06 01:10:47,147 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@7a606260.first_dt to MainDataLoader$InitList@7a606260.last_dt
2023-04-06 01:10:47,147 INFO  [main] MainDataLoader$ - request secs: List(FB)
2023-04-06 01:10:47,147 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2023-04-06 01:10:47,148 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@7a606260.adapter
2023-04-06 01:10:47,584 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 0
2023-04-06 01:10:47,584 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2023-04-06 01:10:51,318 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2023-04-06 01:26:48,522 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3741)
	at org.apache.hadoop.fs.FileSystem$Cache$Key.<init>(FileSystem.java:3736)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3520)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)
	at MainDataLoader$InitList.<init>(MainDataLoader.scala:68)
	at MainDataLoader$.<init>(MainDataLoader.scala:36)
	at MainDataLoader$.<clinit>(MainDataLoader.scala)
	at MainDataLoader.main(MainDataLoader.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 9 common frames omitted
2023-04-06 01:26:48,605 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-04-06 01:26:50,636 INFO  [main] MainDataLoader$ - request data_type: MainDataLoader$InitList@7a606260.data_type
2023-04-06 01:26:50,636 INFO  [main] MainDataLoader$ - request dates: from MainDataLoader$InitList@7a606260.first_dt to MainDataLoader$InitList@7a606260.last_dt
2023-04-06 01:26:50,637 INFO  [main] MainDataLoader$ - request secs: List(FB)
2023-04-06 01:26:50,637 INFO  [main] MainDataLoader$ - request exchanges: List(XNAS)
2023-04-06 01:26:50,637 INFO  [main] MainDataLoader$ - DB adapter: MainDataLoader$InitList@7a606260.adapter
2023-04-06 01:26:51,107 INFO  [main] MainDataLoader$ - Total amount of rows in request set to 0
2023-04-06 01:26:51,107 INFO  [main] MainDataLoader$ - Page amount set to 1 rows
2023-04-06 01:26:54,136 INFO  [main] MainDataLoader$ - try to write 0 rows in m_data.t_source_marketdata
2023-04-06 01:34:05,943 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 01:34:05,945 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 01:34:05,946 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 01:34:05,946 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 01:34:05,946 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 01:34:05,951 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 01:34:05,951 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 01:46:15,043 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 01:46:15,046 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 01:46:15,046 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 01:46:15,046 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 01:46:15,046 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 01:46:15,051 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 01:46:15,051 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 01:46:15,057 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for MainApp$$anon$1@53f3bdbd.StreamType stream
2023-04-06 01:49:26,694 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 01:49:26,699 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 01:49:26,699 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 01:49:26,699 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 01:49:26,699 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 01:49:26,699 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 01:49:26,699 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 01:49:26,705 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 01:49:26,705 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 01:49:26,710 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for MainApp$$anon$1@53f3bdbd.StreamType stream
2023-04-06 01:57:53,766 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 01:57:53,769 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 01:57:53,770 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 01:57:53,770 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 01:57:53,770 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 01:57:53,770 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 01:57:53,770 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 01:57:53,775 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 01:57:53,775 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 01:57:53,779 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 01:57:54,232 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 02:10:32,247 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:10:32,249 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:10:32,250 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:10:32,250 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:10:32,250 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:10:32,250 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:10:32,250 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:10:32,255 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:10:32,255 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:10:32,260 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 02:10:32,764 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 02:10:32,782 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 02:10:32,782 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 02:35:33,341 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:35:33,345 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:35:33,345 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:35:33,345 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:35:33,346 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:35:33,346 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:35:33,346 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:35:33,350 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:35:33,351 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:42:28,429 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:42:28,432 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:42:28,433 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:42:28,433 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:42:28,433 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:42:28,433 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:42:28,433 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:42:28,433 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:42:28,439 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:42:28,440 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:45:25,272 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:45:25,275 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:45:25,276 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:45:25,276 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:45:25,276 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:45:25,276 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:45:25,276 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:45:25,276 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:45:25,283 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:45:25,283 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:47:36,530 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:47:36,533 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:47:36,533 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:47:36,533 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:47:36,533 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:47:36,533 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:47:36,533 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:47:36,533 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:47:36,533 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:47:36,539 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:47:36,540 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:52:20,637 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:52:20,641 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:52:20,641 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:52:20,642 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:52:20,642 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:52:20,643 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:52:20,643 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:52:20,643 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:52:20,643 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:52:20,643 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:52:20,648 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:52:20,648 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:55:18,757 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:55:18,761 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:55:18,761 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:55:18,762 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:55:18,763 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:55:18,763 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:55:18,763 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:55:18,763 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:55:18,763 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:55:18,763 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:55:18,768 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:55:18,768 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:57:25,571 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:57:25,576 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:57:25,576 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:57:25,577 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:57:25,577 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:57:25,577 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:57:25,577 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:57:25,577 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:57:25,577 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:57:25,577 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:57:25,583 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:57:25,583 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:58:21,277 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:58:21,281 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:58:21,281 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:58:21,282 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:58:21,282 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:58:21,283 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:58:21,283 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:58:21,283 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:58:21,283 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:58:21,283 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:58:21,288 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:58:21,288 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:59:05,225 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:59:05,231 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:59:05,231 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 02:59:05,231 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:59:05,231 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:59:05,232 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:59:05,232 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:59:05,232 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:59:05,232 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:59:05,232 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:59:05,232 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:59:05,238 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:59:05,238 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 02:59:45,676 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 02:59:45,681 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 02:59:45,681 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 02:59:45,681 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 02:59:45,682 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 02:59:45,683 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 02:59:45,683 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 02:59:45,683 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 02:59:45,683 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 02:59:45,683 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 02:59:45,683 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 02:59:45,689 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 02:59:45,689 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:00:31,052 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:00:31,054 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:00:31,057 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 03:00:31,057 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:00:31,057 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:00:31,058 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:00:31,059 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:00:31,059 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:00:31,059 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:00:31,059 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:00:31,059 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:00:31,059 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:00:31,066 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:00:31,066 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:00:31,074 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:02:44,092 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:02:44,094 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:02:44,096 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 03:02:44,096 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:02:44,096 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:02:44,098 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:02:44,098 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:02:44,098 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:02:44,098 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:02:44,098 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:02:44,098 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:02:44,098 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:02:44,102 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:02:44,102 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:02:44,108 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:06:55,238 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:06:55,241 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:06:55,253 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 03:06:55,253 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:06:55,253 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:06:55,254 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:06:55,255 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:06:55,255 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:06:55,255 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:06:55,255 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:06:55,255 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:06:55,255 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:06:55,262 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:06:55,262 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:06:55,269 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:11:31,414 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:11:31,417 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:11:31,426 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 03:11:31,427 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:11:31,427 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:11:31,427 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:11:31,428 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:11:31,428 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:11:31,428 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:11:31,428 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:11:31,428 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:11:31,428 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:11:31,432 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:11:31,432 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:11:31,438 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:11:56,807 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:11:56,810 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:11:56,818 INFO  [main] PersistStuct.SettingList$ -  trade params: ()
2023-04-06 03:11:56,818 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:11:56,819 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:11:56,819 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:11:56,820 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:11:56,820 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:11:56,820 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:11:56,820 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:11:56,820 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:11:56,820 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:11:56,826 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:11:56,826 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:11:56,832 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:12:47,105 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:12:47,108 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:12:47,123 INFO  [main] PersistStuct.SettingList$ -  trade params: List(PersistStuct.TradeParams@471a9022, PersistStuct.TradeParams@dc9876b)
2023-04-06 03:12:47,124 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:12:47,124 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:12:47,124 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:12:47,125 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:12:47,125 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:12:47,131 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:14:01,909 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:14:01,912 INFO  [main] PersistStuct.SettingList$ -  exchparams: MOEX
2023-04-06 03:14:01,920 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:14:01,920 INFO  [main] PersistStuct.SettingList$ -  trade size: 2
2023-04-06 03:14:01,920 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:14:01,921 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:14:01,921 INFO  [main] MainApp$ - Passport: trade params - ()
2023-04-06 03:14:01,921 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:14:01,921 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:14:01,921 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:14:01,921 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:14:01,921 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS, MOEX)
2023-04-06 03:14:01,926 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:14:01,926 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:14:01,932 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:20:01,177 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:20:01,182 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:20:01,182 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:20:01,182 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:20:01,183 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:20:01,183 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:20:01,183 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:20:01,183 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:20:01,183 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:20:01,183 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:20:01,183 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:20:01,190 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:20:01,190 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:20:01,197 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:20:45,183 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:20:45,187 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:20:45,187 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:20:45,188 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:20:45,188 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:20:45,188 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:20:45,188 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:20:45,188 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:20:45,188 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:20:45,188 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:20:45,189 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:20:45,194 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:20:45,194 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:20:45,201 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:20:45,708 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:20:45,730 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:20:45,730 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:20:45,730 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:20:45,730 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:21:50,833 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:21:50,836 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:21:50,836 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:21:50,836 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:21:50,837 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:21:50,837 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:21:50,837 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:21:50,837 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:21:50,837 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:21:50,837 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:21:50,837 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:21:50,842 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:21:50,842 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:21:50,848 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:21:51,328 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:21:51,347 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:21:51,347 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:21:51,347 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:21:51,347 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:23:44,371 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:23:44,375 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:23:44,375 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:23:44,375 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:23:44,376 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:23:44,376 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:23:44,376 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:23:44,376 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:23:44,376 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:23:44,376 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:23:44,376 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:23:44,381 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:23:44,381 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:23:44,388 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:23:44,886 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:23:44,906 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:23:44,906 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:23:44,906 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:23:44,906 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:23:44,906 INFO  [main] Streams.WebLoaderEOD - body - {"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-04-06 03:23:44,906 INFO  [main] Streams.WebLoaderEOD - body - StockData(Pagination(1000,0,0,0),List())
2023-04-06 03:25:53,846 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:25:53,850 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:25:53,850 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:25:53,850 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:25:53,851 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:25:53,851 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:25:53,851 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:25:53,851 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:25:53,851 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:25:53,851 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:25:53,851 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:25:53,857 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:25:53,857 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:25:53,862 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:25:54,358 INFO  [main] Streams.WebLoaderEOD - test - WebApi.Request@5f9be66c.rqParams.ticker - WebApi.Request@5f9be66c.rqParams.exchange - WebApi.Request@5f9be66c.rqParams.dt_start
2023-04-06 03:25:54,359 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:25:54,380 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:25:54,380 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:25:54,380 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:25:54,380 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:25:54,380 INFO  [main] Streams.WebLoaderEOD - body - {"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-04-06 03:25:54,380 INFO  [main] Streams.WebLoaderEOD - body - StockData(Pagination(1000,0,0,0),List())
2023-04-06 03:27:19,607 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:27:19,612 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:27:19,612 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:27:19,612 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:27:19,613 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:27:19,613 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:27:19,613 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:27:19,613 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:27:19,613 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:27:19,613 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:27:19,613 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:27:19,619 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:27:19,619 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:27:19,624 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:27:20,153 INFO  [main] Streams.WebLoaderEOD - test - FB - XNAS - WebApi.Request@5f9be66c.rqParams.dt_start
2023-04-06 03:27:20,153 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:27:20,174 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:27:20,174 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:27:20,174 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:27:20,174 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:27:20,174 INFO  [main] Streams.WebLoaderEOD - body - {"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-04-06 03:27:20,174 INFO  [main] Streams.WebLoaderEOD - body - StockData(Pagination(1000,0,0,0),List())
2023-04-06 03:27:55,273 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:27:55,280 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:27:55,280 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:27:55,280 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:27:55,281 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:27:55,281 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:27:55,281 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:27:55,281 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:27:55,281 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:27:55,281 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:27:55,281 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:27:55,286 INFO  [main] PersistStuct.SettingList$ - request secs: List(FB)
2023-04-06 03:27:55,286 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:27:55,291 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:27:55,791 INFO  [main] Streams.WebLoaderEOD - test - FB - XNAS - 2022-07-01 - 2022-08-31
2023-04-06 03:27:55,791 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:27:55,812 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-04-06 03:27:55,812 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:27:55,812 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:27:55,812 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:27:55,812 INFO  [main] Streams.WebLoaderEOD - body - {"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-04-06 03:27:55,812 INFO  [main] Streams.WebLoaderEOD - body - StockData(Pagination(1000,0,0,0),List())
2023-04-06 03:30:05,282 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-04-06 03:30:05,286 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-04-06 03:30:05,286 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-04-06 03:30:05,286 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-04-06 03:30:05,286 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-04-06 03:30:05,286 INFO  [main] MainApp$ - Passport: trade params - 1
2023-04-06 03:30:05,286 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-04-06 03:30:05,286 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-04-06 03:30:05,287 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-04-06 03:30:05,287 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-04-06 03:30:05,287 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-04-06 03:30:05,292 INFO  [main] PersistStuct.SettingList$ - request secs: List(TSLA)
2023-04-06 03:30:05,292 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-04-06 03:30:05,297 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-04-06 03:30:05,813 INFO  [main] Streams.WebLoaderEOD - test - TSLA - XNAS - 2022-07-01 - 2022-08-31
2023-04-06 03:30:05,813 INFO  [main] Streams.WebLoaderEOD - boody {"pagination":{"limit":1000,"offset":0,"count":43,"total":43},"data":[{"open":280.62,"high":281.25,"low":271.81,"close":275.61,"volume":52107337.0,"adj_high":null,"adj_low":null,"adj_close":275.61,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-31T00:00:00+0000"},{"open":287.865,"high":288.48,"low":272.65,"close":277.7,"volume":50541759.0,"adj_high":null,"adj_low":null,"adj_close":277.7,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-30T00:00:00+0000"},{"open":282.83,"high":287.74,"low":280.7,"close":284.82,"volume":41864742.0,"adj_high":null,"adj_low":null,"adj_close":284.82,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-29T00:00:00+0000"},{"open":297.43,"high":302.0,"low":287.47,"close":288.09,"volume":57163953.0,"adj_high":null,"adj_low":null,"adj_close":288.09,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-26T00:00:00+0000"},{"open":302.36,"high":302.96,"low":291.6,"close":296.07,"volume":53230013.0,"adj_high":null,"adj_low":null,"adj_close":296.07,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-25T00:00:00+0000"},{"open":2678.07,"high":2732.82,"low":891.29,"close":891.29,"volume":19030600.0,"adj_high":null,"adj_low":null,"adj_close":297.0967,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-24T00:00:00+0000"},{"open":874.36,"high":896.48,"low":863.77,"close":889.36,"volume":21269900.0,"adj_high":null,"adj_low":null,"adj_close":889.36,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-23T00:00:00+0000"},{"open":875.74,"high":877.2,"low":858.89,"close":869.74,"volume":18546300.0,"adj_high":null,"adj_low":null,"adj_close":869.74,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-22T00:00:00+0000"},{"open":897.0,"high":901.08,"low":877.5,"close":890.0,"volume":20417900.0,"adj_high":null,"adj_low":null,"adj_close":890.0,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-19T00:00:00+0000"},{"open":918.0,"high":919.5,"low":905.56,"close":908.61,"volume":15796700.0,"adj_high":null,"adj_low":null,"adj_close":908.61,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-18T00:00:00+0000"},{"open":910.19,"high":928.97,"low":900.1,"close":911.99,"volume":22921990.0,"adj_high":null,"adj_low":null,"adj_close":911.99,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-17T00:00:00+0000"},{"open":935.0,"high":944.0,"low":908.65,"close":919.69,"volume":29102695.0,"adj_high":null,"adj_low":null,"adj_close":919.69,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-16T00:00:00+0000"},{"open":905.36,"high":939.4,"low":903.69,"close":927.96,"volume":29786389.0,"adj_high":null,"adj_low":null,"adj_close":927.96,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-15T00:00:00+0000"},{"open":868.25,"high":900.48,"low":855.1,"close":900.09,"volume":26443300.0,"adj_high":null,"adj_low":null,"adj_close":900.09,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-12T00:00:00+0000"},{"open":889.54,"high":894.71,"low":857.5,"close":859.89,"volume":23328800.0,"adj_high":null,"adj_low":null,"adj_close":859.89,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-11T00:00:00+0000"},{"open":891.2,"high":892.53,"low":850.11,"close":883.07,"volume":31579300.0,"adj_high":null,"adj_low":null,"adj_close":883.07,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-10T00:00:00+0000"},{"open":870.88,"high":877.1,"low":838.06,"close":850.0,"volume":28748227.0,"adj_high":null,"adj_low":null,"adj_close":850.0,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-09T00:00:00+0000"},{"open":885.0,"high":915.6,"low":867.26,"close":871.27,"volume":32998000.0,"adj_high":null,"adj_low":null,"adj_close":871.27,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-08T00:00:00+0000"},{"open":908.01,"high":913.82,"low":856.63,"close":864.51,"volume":37655300.0,"adj_high":null,"adj_low":null,"adj_close":864.51,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-05T00:00:00+0000"},{"open":933.0,"high":940.82,"low":915.0,"close":925.9,"volume":23770558.0,"adj_high":null,"adj_low":null,"adj_close":925.9,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-04T00:00:00+0000"},{"open":915.0,"high":928.65,"low":903.45,"close":922.19,"volume":26646100.0,"adj_high":null,"adj_low":null,"adj_close":922.19,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-03T00:00:00+0000"},{"open":882.01,"high":923.5,"low":878.0,"close":901.76,"volume":31794000.0,"adj_high":null,"adj_low":null,"adj_close":901.76,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-02T00:00:00+0000"},{"open":903.83,"high":935.63,"low":885.0,"close":891.83,"volume":38952200.0,"adj_high":null,"adj_low":null,"adj_close":891.83,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-01T00:00:00+0000"},{"open":842.1,"high":894.96,"low":837.3,"close":891.45,"volume":31699000.0,"adj_high":null,"adj_low":null,"adj_close":891.45,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-29T00:00:00+0000"},{"open":840.2,"high":849.9,"low":818.43,"close":842.7,"volume":28240997.0,"adj_high":null,"adj_low":null,"adj_close":842.7,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-28T00:00:00+0000"},{"open":791.43,"high":827.7768,"low":785.3701,"close":824.46,"volume":28999009.0,"adj_high":null,"adj_low":null,"adj_close":824.46,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-27T00:00:00+0000"},{"open":799.54,"high":801.9299,"low":768.88,"close":776.58,"volume":22011302.0,"adj_high":null,"adj_low":null,"adj_close":776.58,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-26T00:00:00+0000"},{"open":816.65,"high":822.44,"low":802.2,"close":805.3,"volume":21357835.0,"adj_high":null,"adj_low":null,"adj_close":805.3,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-25T00:00:00+0000"},{"open":828.66,"high":842.36,"low":812.14,"close":816.73,"volume":34421200.0,"adj_high":null,"adj_low":null,"adj_close":816.73,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-22T00:00:00+0000"},{"open":765.32,"high":819.8,"low":764.6,"close":815.12,"volume":46871146.0,"adj_high":null,"adj_low":null,"adj_close":815.12,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-21T00:00:00+0000"},{"open":740.35,"high":751.9528,"low":730.449,"close":742.5,"volume":29621363.0,"adj_high":null,"adj_low":null,"adj_close":742.5,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-20T00:00:00+0000"},{"open":735.0,"high":741.42,"low":710.97,"close":736.59,"volume":26446734.0,"adj_high":null,"adj_low":null,"adj_close":736.59,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-19T00:00:00+0000"},{"open":734.81,"high":751.535,"low":718.86,"close":721.64,"volume":27413327.0,"adj_high":null,"adj_low":null,"adj_close":721.64,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-18T00:00:00+0000"},{"open":720.0,"high":730.87,"low":710.67,"close":720.2,"volume":23165500.0,"adj_high":null,"adj_low":null,"adj_close":720.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-15T00:00:00+0000"},{"open":704.69,"high":715.9599,"low":688.06,"close":714.94,"volume":25930935.0,"adj_high":null,"adj_low":null,"adj_close":714.94,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-14T00:00:00+0000"},{"open":676.5,"high":726.1799,"low":675.11,"close":711.12,"volume":32651499.0,"adj_high":null,"adj_low":null,"adj_close":711.12,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-13T00:00:00+0000"},{"open":710.54,"high":719.31,"low":685.105,"close":699.21,"volume":29310320.0,"adj_high":null,"adj_low":null,"adj_close":699.21,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-12T00:00:00+0000"},{"open":756.31,"high":759.19,"low":700.885,"close":703.03,"volume":32830749.0,"adj_high":null,"adj_low":null,"adj_close":703.03,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-11T00:00:00+0000"},{"open":727.0,"high":764.94,"low":723.48,"close":752.29,"volume":33343700.0,"adj_high":null,"adj_low":null,"adj_close":752.29,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-08T00:00:00+0000"},{"open":701.76,"high":736.0438,"low":696.63,"close":733.63,"volume":27310230.0,"adj_high":null,"adj_low":null,"adj_close":733.63,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-07T00:00:00+0000"},{"open":692.34,"high":703.69,"low":681.56,"close":695.2,"volume":23502106.0,"adj_high":null,"adj_low":null,"adj_close":695.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-06T00:00:00+0000"},{"open":669.0,"high":699.44,"low":648.5001,"close":699.2,"volume":28259704.0,"adj_high":null,"adj_low":null,"adj_close":699.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-05T00:00:00+0000"},{"open":681.0,"high":690.69,"low":666.36,"close":681.79,"volume":24781500.0,"adj_high":null,"adj_low":null,"adj_close":681.79,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-01T00:00:00+0000"}]}
2023-04-06 03:30:05,813 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-04-06 03:30:05,835 INFO  [main] WebApi.Request - Total amount of rows in request set to 43
2023-04-06 03:30:05,835 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-04-06 03:30:05,835 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-04-06 03:30:05,835 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-04-06 03:30:05,835 INFO  [main] Streams.WebLoaderEOD - body - {"pagination":{"limit":1000,"offset":0,"count":43,"total":43},"data":[{"open":280.62,"high":281.25,"low":271.81,"close":275.61,"volume":52107337.0,"adj_high":null,"adj_low":null,"adj_close":275.61,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-31T00:00:00+0000"},{"open":287.865,"high":288.48,"low":272.65,"close":277.7,"volume":50541759.0,"adj_high":null,"adj_low":null,"adj_close":277.7,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-30T00:00:00+0000"},{"open":282.83,"high":287.74,"low":280.7,"close":284.82,"volume":41864742.0,"adj_high":null,"adj_low":null,"adj_close":284.82,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-29T00:00:00+0000"},{"open":297.43,"high":302.0,"low":287.47,"close":288.09,"volume":57163953.0,"adj_high":null,"adj_low":null,"adj_close":288.09,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-26T00:00:00+0000"},{"open":302.36,"high":302.96,"low":291.6,"close":296.07,"volume":53230013.0,"adj_high":null,"adj_low":null,"adj_close":296.07,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-25T00:00:00+0000"},{"open":2678.07,"high":2732.82,"low":891.29,"close":891.29,"volume":19030600.0,"adj_high":null,"adj_low":null,"adj_close":297.0967,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-24T00:00:00+0000"},{"open":874.36,"high":896.48,"low":863.77,"close":889.36,"volume":21269900.0,"adj_high":null,"adj_low":null,"adj_close":889.36,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-23T00:00:00+0000"},{"open":875.74,"high":877.2,"low":858.89,"close":869.74,"volume":18546300.0,"adj_high":null,"adj_low":null,"adj_close":869.74,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-22T00:00:00+0000"},{"open":897.0,"high":901.08,"low":877.5,"close":890.0,"volume":20417900.0,"adj_high":null,"adj_low":null,"adj_close":890.0,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-19T00:00:00+0000"},{"open":918.0,"high":919.5,"low":905.56,"close":908.61,"volume":15796700.0,"adj_high":null,"adj_low":null,"adj_close":908.61,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-18T00:00:00+0000"},{"open":910.19,"high":928.97,"low":900.1,"close":911.99,"volume":22921990.0,"adj_high":null,"adj_low":null,"adj_close":911.99,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-17T00:00:00+0000"},{"open":935.0,"high":944.0,"low":908.65,"close":919.69,"volume":29102695.0,"adj_high":null,"adj_low":null,"adj_close":919.69,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-16T00:00:00+0000"},{"open":905.36,"high":939.4,"low":903.69,"close":927.96,"volume":29786389.0,"adj_high":null,"adj_low":null,"adj_close":927.96,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-15T00:00:00+0000"},{"open":868.25,"high":900.48,"low":855.1,"close":900.09,"volume":26443300.0,"adj_high":null,"adj_low":null,"adj_close":900.09,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-12T00:00:00+0000"},{"open":889.54,"high":894.71,"low":857.5,"close":859.89,"volume":23328800.0,"adj_high":null,"adj_low":null,"adj_close":859.89,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-11T00:00:00+0000"},{"open":891.2,"high":892.53,"low":850.11,"close":883.07,"volume":31579300.0,"adj_high":null,"adj_low":null,"adj_close":883.07,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-10T00:00:00+0000"},{"open":870.88,"high":877.1,"low":838.06,"close":850.0,"volume":28748227.0,"adj_high":null,"adj_low":null,"adj_close":850.0,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-09T00:00:00+0000"},{"open":885.0,"high":915.6,"low":867.26,"close":871.27,"volume":32998000.0,"adj_high":null,"adj_low":null,"adj_close":871.27,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-08T00:00:00+0000"},{"open":908.01,"high":913.82,"low":856.63,"close":864.51,"volume":37655300.0,"adj_high":null,"adj_low":null,"adj_close":864.51,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-05T00:00:00+0000"},{"open":933.0,"high":940.82,"low":915.0,"close":925.9,"volume":23770558.0,"adj_high":null,"adj_low":null,"adj_close":925.9,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-04T00:00:00+0000"},{"open":915.0,"high":928.65,"low":903.45,"close":922.19,"volume":26646100.0,"adj_high":null,"adj_low":null,"adj_close":922.19,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-03T00:00:00+0000"},{"open":882.01,"high":923.5,"low":878.0,"close":901.76,"volume":31794000.0,"adj_high":null,"adj_low":null,"adj_close":901.76,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-02T00:00:00+0000"},{"open":903.83,"high":935.63,"low":885.0,"close":891.83,"volume":38952200.0,"adj_high":null,"adj_low":null,"adj_close":891.83,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-08-01T00:00:00+0000"},{"open":842.1,"high":894.96,"low":837.3,"close":891.45,"volume":31699000.0,"adj_high":null,"adj_low":null,"adj_close":891.45,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-29T00:00:00+0000"},{"open":840.2,"high":849.9,"low":818.43,"close":842.7,"volume":28240997.0,"adj_high":null,"adj_low":null,"adj_close":842.7,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-28T00:00:00+0000"},{"open":791.43,"high":827.7768,"low":785.3701,"close":824.46,"volume":28999009.0,"adj_high":null,"adj_low":null,"adj_close":824.46,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-27T00:00:00+0000"},{"open":799.54,"high":801.9299,"low":768.88,"close":776.58,"volume":22011302.0,"adj_high":null,"adj_low":null,"adj_close":776.58,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-26T00:00:00+0000"},{"open":816.65,"high":822.44,"low":802.2,"close":805.3,"volume":21357835.0,"adj_high":null,"adj_low":null,"adj_close":805.3,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-25T00:00:00+0000"},{"open":828.66,"high":842.36,"low":812.14,"close":816.73,"volume":34421200.0,"adj_high":null,"adj_low":null,"adj_close":816.73,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-22T00:00:00+0000"},{"open":765.32,"high":819.8,"low":764.6,"close":815.12,"volume":46871146.0,"adj_high":null,"adj_low":null,"adj_close":815.12,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-21T00:00:00+0000"},{"open":740.35,"high":751.9528,"low":730.449,"close":742.5,"volume":29621363.0,"adj_high":null,"adj_low":null,"adj_close":742.5,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-20T00:00:00+0000"},{"open":735.0,"high":741.42,"low":710.97,"close":736.59,"volume":26446734.0,"adj_high":null,"adj_low":null,"adj_close":736.59,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-19T00:00:00+0000"},{"open":734.81,"high":751.535,"low":718.86,"close":721.64,"volume":27413327.0,"adj_high":null,"adj_low":null,"adj_close":721.64,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-18T00:00:00+0000"},{"open":720.0,"high":730.87,"low":710.67,"close":720.2,"volume":23165500.0,"adj_high":null,"adj_low":null,"adj_close":720.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-15T00:00:00+0000"},{"open":704.69,"high":715.9599,"low":688.06,"close":714.94,"volume":25930935.0,"adj_high":null,"adj_low":null,"adj_close":714.94,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-14T00:00:00+0000"},{"open":676.5,"high":726.1799,"low":675.11,"close":711.12,"volume":32651499.0,"adj_high":null,"adj_low":null,"adj_close":711.12,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-13T00:00:00+0000"},{"open":710.54,"high":719.31,"low":685.105,"close":699.21,"volume":29310320.0,"adj_high":null,"adj_low":null,"adj_close":699.21,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-12T00:00:00+0000"},{"open":756.31,"high":759.19,"low":700.885,"close":703.03,"volume":32830749.0,"adj_high":null,"adj_low":null,"adj_close":703.03,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-11T00:00:00+0000"},{"open":727.0,"high":764.94,"low":723.48,"close":752.29,"volume":33343700.0,"adj_high":null,"adj_low":null,"adj_close":752.29,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-08T00:00:00+0000"},{"open":701.76,"high":736.0438,"low":696.63,"close":733.63,"volume":27310230.0,"adj_high":null,"adj_low":null,"adj_close":733.63,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-07T00:00:00+0000"},{"open":692.34,"high":703.69,"low":681.56,"close":695.2,"volume":23502106.0,"adj_high":null,"adj_low":null,"adj_close":695.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-06T00:00:00+0000"},{"open":669.0,"high":699.44,"low":648.5001,"close":699.2,"volume":28259704.0,"adj_high":null,"adj_low":null,"adj_close":699.2,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-05T00:00:00+0000"},{"open":681.0,"high":690.69,"low":666.36,"close":681.79,"volume":24781500.0,"adj_high":null,"adj_low":null,"adj_close":681.79,"adj_open":null,"adj_volume":null,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2022-07-01T00:00:00+0000"}]}
2023-04-06 03:30:05,835 INFO  [main] Streams.WebLoaderEOD - body - StockData(Pagination(1000,0,43,43),List(DataEoD(280.62,281.25,271.81,275.61,5.2107337E7,None,None,Some(275.61),None,None,1.0,0.0,TSLA,XNAS,2022-08-31T00:00:00+0000), DataEoD(287.865,288.48,272.65,277.7,5.0541759E7,None,None,Some(277.7),None,None,1.0,0.0,TSLA,XNAS,2022-08-30T00:00:00+0000), DataEoD(282.83,287.74,280.7,284.82,4.1864742E7,None,None,Some(284.82),None,None,1.0,0.0,TSLA,XNAS,2022-08-29T00:00:00+0000), DataEoD(297.43,302.0,287.47,288.09,5.7163953E7,None,None,Some(288.09),None,None,1.0,0.0,TSLA,XNAS,2022-08-26T00:00:00+0000), DataEoD(302.36,302.96,291.6,296.07,5.3230013E7,None,None,Some(296.07),None,None,1.0,0.0,TSLA,XNAS,2022-08-25T00:00:00+0000), DataEoD(2678.07,2732.82,891.29,891.29,1.90306E7,None,None,Some(297.0967),None,None,1.0,0.0,TSLA,XNAS,2022-08-24T00:00:00+0000), DataEoD(874.36,896.48,863.77,889.36,2.12699E7,None,None,Some(889.36),None,None,1.0,0.0,TSLA,XNAS,2022-08-23T00:00:00+0000), DataEoD(875.74,877.2,858.89,869.74,1.85463E7,None,None,Some(869.74),None,None,1.0,0.0,TSLA,XNAS,2022-08-22T00:00:00+0000), DataEoD(897.0,901.08,877.5,890.0,2.04179E7,None,None,Some(890.0),None,None,1.0,0.0,TSLA,XNAS,2022-08-19T00:00:00+0000), DataEoD(918.0,919.5,905.56,908.61,1.57967E7,None,None,Some(908.61),None,None,1.0,0.0,TSLA,XNAS,2022-08-18T00:00:00+0000), DataEoD(910.19,928.97,900.1,911.99,2.292199E7,None,None,Some(911.99),None,None,1.0,0.0,TSLA,XNAS,2022-08-17T00:00:00+0000), DataEoD(935.0,944.0,908.65,919.69,2.9102695E7,None,None,Some(919.69),None,None,1.0,0.0,TSLA,XNAS,2022-08-16T00:00:00+0000), DataEoD(905.36,939.4,903.69,927.96,2.9786389E7,None,None,Some(927.96),None,None,1.0,0.0,TSLA,XNAS,2022-08-15T00:00:00+0000), DataEoD(868.25,900.48,855.1,900.09,2.64433E7,None,None,Some(900.09),None,None,1.0,0.0,TSLA,XNAS,2022-08-12T00:00:00+0000), DataEoD(889.54,894.71,857.5,859.89,2.33288E7,None,None,Some(859.89),None,None,1.0,0.0,TSLA,XNAS,2022-08-11T00:00:00+0000), DataEoD(891.2,892.53,850.11,883.07,3.15793E7,None,None,Some(883.07),None,None,1.0,0.0,TSLA,XNAS,2022-08-10T00:00:00+0000), DataEoD(870.88,877.1,838.06,850.0,2.8748227E7,None,None,Some(850.0),None,None,1.0,0.0,TSLA,XNAS,2022-08-09T00:00:00+0000), DataEoD(885.0,915.6,867.26,871.27,3.2998E7,None,None,Some(871.27),None,None,1.0,0.0,TSLA,XNAS,2022-08-08T00:00:00+0000), DataEoD(908.01,913.82,856.63,864.51,3.76553E7,None,None,Some(864.51),None,None,1.0,0.0,TSLA,XNAS,2022-08-05T00:00:00+0000), DataEoD(933.0,940.82,915.0,925.9,2.3770558E7,None,None,Some(925.9),None,None,1.0,0.0,TSLA,XNAS,2022-08-04T00:00:00+0000), DataEoD(915.0,928.65,903.45,922.19,2.66461E7,None,None,Some(922.19),None,None,1.0,0.0,TSLA,XNAS,2022-08-03T00:00:00+0000), DataEoD(882.01,923.5,878.0,901.76,3.1794E7,None,None,Some(901.76),None,None,1.0,0.0,TSLA,XNAS,2022-08-02T00:00:00+0000), DataEoD(903.83,935.63,885.0,891.83,3.89522E7,None,None,Some(891.83),None,None,1.0,0.0,TSLA,XNAS,2022-08-01T00:00:00+0000), DataEoD(842.1,894.96,837.3,891.45,3.1699E7,None,None,Some(891.45),None,None,1.0,0.0,TSLA,XNAS,2022-07-29T00:00:00+0000), DataEoD(840.2,849.9,818.43,842.7,2.8240997E7,None,None,Some(842.7),None,None,1.0,0.0,TSLA,XNAS,2022-07-28T00:00:00+0000), DataEoD(791.43,827.7768,785.3701,824.46,2.8999009E7,None,None,Some(824.46),None,None,1.0,0.0,TSLA,XNAS,2022-07-27T00:00:00+0000), DataEoD(799.54,801.9299,768.88,776.58,2.2011302E7,None,None,Some(776.58),None,None,1.0,0.0,TSLA,XNAS,2022-07-26T00:00:00+0000), DataEoD(816.65,822.44,802.2,805.3,2.1357835E7,None,None,Some(805.3),None,None,1.0,0.0,TSLA,XNAS,2022-07-25T00:00:00+0000), DataEoD(828.66,842.36,812.14,816.73,3.44212E7,None,None,Some(816.73),None,None,1.0,0.0,TSLA,XNAS,2022-07-22T00:00:00+0000), DataEoD(765.32,819.8,764.6,815.12,4.6871146E7,None,None,Some(815.12),None,None,1.0,0.0,TSLA,XNAS,2022-07-21T00:00:00+0000), DataEoD(740.35,751.9528,730.449,742.5,2.9621363E7,None,None,Some(742.5),None,None,1.0,0.0,TSLA,XNAS,2022-07-20T00:00:00+0000), DataEoD(735.0,741.42,710.97,736.59,2.6446734E7,None,None,Some(736.59),None,None,1.0,0.0,TSLA,XNAS,2022-07-19T00:00:00+0000), DataEoD(734.81,751.535,718.86,721.64,2.7413327E7,None,None,Some(721.64),None,None,1.0,0.0,TSLA,XNAS,2022-07-18T00:00:00+0000), DataEoD(720.0,730.87,710.67,720.2,2.31655E7,None,None,Some(720.2),None,None,1.0,0.0,TSLA,XNAS,2022-07-15T00:00:00+0000), DataEoD(704.69,715.9599,688.06,714.94,2.5930935E7,None,None,Some(714.94),None,None,1.0,0.0,TSLA,XNAS,2022-07-14T00:00:00+0000), DataEoD(676.5,726.1799,675.11,711.12,3.2651499E7,None,None,Some(711.12),None,None,1.0,0.0,TSLA,XNAS,2022-07-13T00:00:00+0000), DataEoD(710.54,719.31,685.105,699.21,2.931032E7,None,None,Some(699.21),None,None,1.0,0.0,TSLA,XNAS,2022-07-12T00:00:00+0000), DataEoD(756.31,759.19,700.885,703.03,3.2830749E7,None,None,Some(703.03),None,None,1.0,0.0,TSLA,XNAS,2022-07-11T00:00:00+0000), DataEoD(727.0,764.94,723.48,752.29,3.33437E7,None,None,Some(752.29),None,None,1.0,0.0,TSLA,XNAS,2022-07-08T00:00:00+0000), DataEoD(701.76,736.0438,696.63,733.63,2.731023E7,None,None,Some(733.63),None,None,1.0,0.0,TSLA,XNAS,2022-07-07T00:00:00+0000), DataEoD(692.34,703.69,681.56,695.2,2.3502106E7,None,None,Some(695.2),None,None,1.0,0.0,TSLA,XNAS,2022-07-06T00:00:00+0000), DataEoD(669.0,699.44,648.5001,699.2,2.8259704E7,None,None,Some(699.2),None,None,1.0,0.0,TSLA,XNAS,2022-07-05T00:00:00+0000), DataEoD(681.0,690.69,666.36,681.79,2.47815E7,None,None,Some(681.79),None,None,1.0,0.0,TSLA,XNAS,2022-07-01T00:00:00+0000)))
2023-11-16 20:03:01,879 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-16 20:03:01,884 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-16 20:03:01,884 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-16 20:03:01,884 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-11-16 20:03:01,885 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-11-16 20:03:01,885 INFO  [main] MainApp$ - Passport: trade params - 1
2023-11-16 20:03:01,885 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-11-16 20:03:01,885 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-11-16 20:03:01,885 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-11-16 20:03:01,885 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-11-16 20:03:01,885 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-11-16 20:03:01,891 INFO  [main] PersistStuct.SettingList$ - request secs: List(TSLA)
2023-11-16 20:03:01,891 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-11-16 20:03:01,897 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-11-16 20:09:59,893 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-16 20:09:59,897 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-16 20:09:59,898 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-16 20:09:59,898 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-11-16 20:09:59,898 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-11-16 20:09:59,898 INFO  [main] MainApp$ - Passport: trade params - 1
2023-11-16 20:09:59,898 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-11-16 20:09:59,899 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-11-16 20:09:59,899 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-11-16 20:09:59,899 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-11-16 20:09:59,899 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-11-16 20:09:59,904 INFO  [main] PersistStuct.SettingList$ - request secs: List(TSLA)
2023-11-16 20:09:59,904 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-11-16 20:09:59,909 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-11-16 20:09:59,910 INFO  [main] ExternalWorker.ExtFactory -  Create ExtConsumer for EOD stream
2023-11-16 20:10:00,510 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-16 20:10:00,531 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-11-16 20:10:00,531 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-11-16 20:10:00,531 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-11-16 20:10:00,531 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-11-16 20:14:56,720 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-16 20:14:56,723 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-16 20:14:56,723 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-16 20:14:56,724 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for WEBAPI
2023-11-16 20:14:56,724 INFO  [main] ExternalWorker.ExtFactory -  Create ExtSystem for POSTGRES
2023-11-16 20:14:56,724 INFO  [main] MainApp$ - Passport: trade params - 1
2023-11-16 20:14:56,724 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-11-16 20:14:56,724 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-11-16 20:14:56,724 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-11-16 20:14:56,724 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-11-16 20:14:56,724 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-11-16 20:14:56,729 INFO  [main] PersistStuct.SettingList$ - request secs: List(TSLA)
2023-11-16 20:14:56,730 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-11-16 20:14:56,734 INFO  [main] ExternalWorker.ExtFactory -  Create ExtProducer for EOD stream
2023-11-16 20:14:56,735 INFO  [main] ExternalWorker.ExtFactory -  Create ExtConsumer for EOD stream
2023-11-16 20:14:57,250 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-16 20:14:57,275 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-11-16 20:14:57,275 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-11-16 20:14:57,275 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-11-16 20:14:57,275 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone class is ready
2023-11-16 20:14:57,278 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-11-19 17:46:31,398 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ -  datatype: EOD
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ -  source system: WEBAPI
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ -  target system: POSTGRES
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ - --------------------------------------------------------
2023-11-19 17:46:31,402 INFO  [main] PersistStuct.SettingList$ - request exchanges: List(XNAS)
2023-11-19 17:46:31,407 INFO  [main] PersistStuct.SettingList$ - request secs: List(TSLA)
2023-11-19 17:46:31,407 INFO  [main] PersistStuct.SettingList$ - request dates: from 2022-07-01  to 2022-07-01
2023-11-19 17:46:31,407 INFO  [main] PersistStuct.SettingList$ - request data_type: List(PersistStuct.TradeParams@54e041a4).
2023-11-22 20:35:10,900 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:35:10,904 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:35:10,904 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:35:10,905 INFO  [main] PersistStuct.ParamList -  trade params: XNAS
2023-11-22 20:35:10,905 INFO  [main] PersistStuct.ParamList -  trade size: 1
2023-11-22 20:37:05,132 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:37:05,135 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:37:05,136 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:38:01,836 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:38:01,840 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:38:01,840 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:38:18,194 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:38:18,198 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:38:18,198 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:41:10,701 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:41:10,703 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:41:10,703 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:43:33,619 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:43:33,623 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:43:33,624 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:49:27,544 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:49:27,549 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:49:27,549 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:49:54,184 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:49:54,188 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:49:54,188 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:53:41,635 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:53:41,638 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:53:41,638 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:54:02,294 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:54:02,297 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:54:02,297 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:57:08,072 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:57:08,075 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:57:08,075 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:57:42,496 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:57:42,499 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:57:42,500 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:59:18,803 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:59:18,806 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:59:18,807 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 20:59:47,852 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 20:59:47,855 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 20:59:47,855 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:00:19,384 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:00:19,388 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:00:19,388 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:00:36,841 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:00:36,845 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:00:36,845 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:06:10,458 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:06:10,463 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:06:10,463 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:08:32,798 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:08:32,801 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:08:32,801 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:08:55,656 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:08:55,660 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:08:55,660 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:11:02,783 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:11:02,787 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:11:02,787 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 21:12:00,008 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 21:12:00,012 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 21:12:00,012 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:13:54,653 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:13:54,656 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:13:54,656 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:24:47,578 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:24:47,581 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:24:47,581 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:26:33,821 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:26:33,824 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:26:33,824 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:27:11,727 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:27:11,730 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:27:11,731 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:36:41,301 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:36:41,305 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:36:41,305 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:37:15,034 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:37:15,037 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:37:15,037 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:37:51,573 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:37:51,577 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:37:51,577 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:39:21,015 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:39:21,019 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:39:21,019 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:54:16,886 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:54:16,890 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:54:16,890 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-22 22:54:26,415 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-22 22:54:26,420 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-22 22:54:26,420 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-23 21:20:35,110 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-23 21:20:35,113 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-23 21:20:35,114 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-23 21:21:56,502 INFO  [main] PersistStuct.SettingList$ -  exchparams: XNAS
2023-11-23 21:21:56,505 INFO  [main] PersistStuct.SettingList$ -  trade params: XNAS
2023-11-23 21:21:56,505 INFO  [main] PersistStuct.SettingList$ -  trade size: 1
2023-11-23 21:54:45,666 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 21:54:45,668 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:01:46,520 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD
2023-11-23 22:01:46,522 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:01:46,523 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:44:03,128 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD
2023-11-23 22:44:03,131 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:44:03,132 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:47:15,740 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD
2023-11-23 22:47:15,742 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:47:15,743 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:49:31,203 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD
2023-11-23 22:49:31,205 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:49:31,206 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:49:46,628 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD
2023-11-23 22:49:46,630 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:49:46,630 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:53:06,548 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:53:06,550 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:53:06,551 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:53:54,664 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:53:54,667 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:53:54,668 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:54:45,929 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:54:45,931 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:54:45,932 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:55:03,398 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:55:03,401 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:55:03,401 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:57:00,117 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:57:00,119 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:57:00,120 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:57:45,076 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:57:45,079 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:57:45,079 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 22:59:20,316 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 22:59:20,318 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 22:59:20,318 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 23:00:08,239 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 23:00:08,241 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 23:00:08,242 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 23:03:03,228 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-23 23:03:03,231 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 23:03:03,231 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 23:06:26,708 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 23:06:26,711 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 23:07:30,024 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-23 23:07:30,028 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-23 23:07:30,638 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-30 22:56:59,519 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-30 22:56:59,522 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-30 22:57:00,114 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-30 23:03:01,130 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-30 23:03:01,133 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-30 23:03:01,674 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-30 23:03:01,675 INFO  [main] Streams.WebLoaderEOD - web respone body:{"error":{"code":"validation_error","message":"You have to specify at least one symbol and not more than 100"}}
2023-11-30 23:15:48,231 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-30 23:15:48,233 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-30 23:15:48,234 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-30 23:15:48,747 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-30 23:15:48,748 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-11-30 23:15:48,776 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-11-30 23:15:48,776 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-11-30 23:15:48,776 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-11-30 23:15:48,776 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body 
2023-11-30 23:15:48,777 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-11-30 23:44:01,556 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-11-30 23:44:01,558 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-11-30 23:44:01,559 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-11-30 23:44:01,559 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-11-30 23:44:02,084 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-11-30 23:44:02,087 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-11-30 23:44:02,104 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-11-30 23:44:02,104 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-11-30 23:44:02,104 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready
2023-11-30 23:44:02,104 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-11-30 23:44:02,105 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 00:07:15,044 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:07:15,047 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:07:15,048 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:07:15,467 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:16)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 18 common frames omitted
2023-12-01 00:07:15,481 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:07:15,600 ERROR [main] org.apache.spark.SparkContext - Error initializing SparkContext.
org.apache.spark.SparkException: A master URL must be set in your configuration
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:394)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:16)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
2023-12-01 00:09:36,362 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:09:36,364 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:09:36,364 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:09:36,705 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:18)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 18 common frames omitted
2023-12-01 00:09:36,719 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:09:38,347 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 00:09:38,796 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 00:09:38,798 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-12-01 00:09:38,809 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-01 00:09:38,809 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 00:09:38,810 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready List()
2023-12-01 00:09:38,810 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 00:09:42,015 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 00:23:18,890 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:23:18,892 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:23:18,893 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:23:19,231 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:18)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 18 common frames omitted
2023-12-01 00:23:19,241 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:23:20,569 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 00:23:21,048 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 00:23:21,051 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-12-01 00:23:21,061 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-01 00:23:21,061 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 00:23:21,061 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready List()
2023-12-01 00:23:21,061 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 00:23:23,766 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 00:26:08,345 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:26:08,346 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:26:08,347 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:26:08,688 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:18)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 18 common frames omitted
2023-12-01 00:26:08,698 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:26:10,060 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 00:26:10,461 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 00:26:10,463 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-12-01 00:26:10,474 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-01 00:26:10,474 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 00:26:10,474 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready List()
2023-12-01 00:26:10,474 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 00:33:29,893 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:33:29,896 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:33:29,897 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:33:30,291 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:33:31,693 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 00:33:32,087 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 00:33:32,088 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-12-01 00:33:32,099 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-01 00:33:32,099 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 00:33:32,099 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready List()
2023-12-01 00:33:32,099 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 00:35:07,829 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 00:35:07,832 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 00:35:07,832 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 00:35:08,204 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 00:35:09,608 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 00:35:09,998 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 00:35:10,000 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":0,"total":0},"data":[]}
2023-12-01 00:35:10,010 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-01 00:35:10,010 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 00:35:10,010 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready List()
2023-12-01 00:35:10,010 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 01:35:51,200 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:35:51,202 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:35:51,204 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:35:51,552 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:35:52,928 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:35:53,352 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone body is ready
2023-12-01 01:35:53,354 INFO  [main] Streams.WebLoaderEOD - web respone body:{"pagination":{"limit":1000,"offset":0,"count":22,"total":22},"data":[{"open":196.12,"high":202.8,"low":194.07,"close":200.84,"volume":118068273.0,"adj_high":202.8,"adj_low":194.07,"adj_close":200.84,"adj_open":196.12,"adj_volume":118068273.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-31T00:00:00+0000"},{"open":209.28,"high":210.88,"low":194.67,"close":197.36,"volume":136448167.0,"adj_high":210.88,"adj_low":194.67,"adj_close":197.36,"adj_open":209.28,"adj_volume":136448167.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-30T00:00:00+0000"},{"open":210.6,"high":212.41,"low":205.77,"close":207.3,"volume":94881173.0,"adj_high":212.41,"adj_low":205.77,"adj_close":207.3,"adj_open":210.6,"adj_volume":94881173.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-27T00:00:00+0000"},{"open":211.32,"high":214.8,"low":204.88,"close":205.76,"volume":115112635.0,"adj_high":214.8,"adj_low":204.88,"adj_close":205.76,"adj_open":211.32,"adj_volume":115112635.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-26T00:00:00+0000"},{"open":215.88,"high":220.1,"low":212.2,"close":212.42,"volume":107065087.0,"adj_high":220.1,"adj_low":212.2,"adj_close":212.42,"adj_open":215.88,"adj_volume":107065087.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-25T00:00:00+0000"},{"open":216.5,"high":222.05,"low":214.11,"close":216.52,"volume":118231113.0,"adj_high":222.05,"adj_low":214.11,"adj_close":216.52,"adj_open":216.5,"adj_volume":118231113.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-24T00:00:00+0000"},{"open":210.0,"high":216.98,"low":202.51,"close":212.08,"volume":150683368.0,"adj_high":216.98,"adj_low":202.51,"adj_close":212.08,"adj_open":210.0,"adj_volume":150683368.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-23T00:00:00+0000"},{"open":217.01,"high":218.8606,"low":210.42,"close":211.99,"volume":138010095.0,"adj_high":218.8606,"adj_low":210.42,"adj_close":211.99,"adj_open":217.01,"adj_volume":138010095.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-20T00:00:00+0000"},{"open":225.95,"high":230.61,"low":216.78,"close":220.11,"volume":170772713.0,"adj_high":230.61,"adj_low":216.78,"adj_close":220.11,"adj_open":225.95,"adj_volume":170772713.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-19T00:00:00+0000"},{"open":252.7,"high":254.63,"low":242.08,"close":242.68,"volume":125147846.0,"adj_high":254.63,"adj_low":242.08,"adj_close":242.68,"adj_open":252.7,"adj_volume":125147846.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-18T00:00:00+0000"},{"open":250.1,"high":257.183,"low":247.08,"close":254.85,"volume":93562909.0,"adj_high":257.183,"adj_low":247.08,"adj_close":254.85,"adj_open":250.1,"adj_volume":93562909.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-17T00:00:00+0000"},{"open":250.05,"high":255.3999,"low":248.48,"close":253.92,"volume":88917176.0,"adj_high":255.3999,"adj_low":248.48,"adj_close":253.92,"adj_open":250.05,"adj_volume":88917176.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-16T00:00:00+0000"},{"open":258.9,"high":259.6,"low":250.22,"close":251.12,"volume":102296786.0,"adj_high":259.6,"adj_low":250.22,"adj_close":251.12,"adj_open":258.9,"adj_volume":102296786.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-13T00:00:00+0000"},{"open":262.92,"high":265.41,"low":256.6307,"close":258.87,"volume":111508114.0,"adj_high":265.41,"adj_low":256.6307,"adj_close":258.87,"adj_open":262.92,"adj_volume":111508114.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-12T00:00:00+0000"},{"open":266.2,"high":268.6,"low":260.9,"close":262.99,"volume":103706266.0,"adj_high":268.6,"adj_low":260.9,"adj_close":262.99,"adj_open":266.2,"adj_volume":103706266.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-11T00:00:00+0000"},{"open":257.78,"high":268.94,"low":257.65,"close":263.62,"volume":122656030.0,"adj_high":268.94,"adj_low":257.65,"adj_close":263.62,"adj_open":257.75,"adj_volume":122656030.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-10T00:00:00+0000"},{"open":255.21,"high":261.36,"low":252.05,"close":259.67,"volume":101377947.0,"adj_high":261.36,"adj_low":252.05,"adj_close":259.67,"adj_open":255.31,"adj_volume":101377947.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-09T00:00:00+0000"},{"open":253.98,"high":261.65,"low":250.65,"close":260.53,"volume":118121812.0,"adj_high":261.65,"adj_low":250.65,"adj_close":260.53,"adj_open":253.98,"adj_volume":118121812.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-06T00:00:00+0000"},{"open":260.0,"high":263.6,"low":256.25,"close":260.05,"volume":119159214.0,"adj_high":263.6,"adj_low":256.25,"adj_close":260.05,"adj_open":260.0,"adj_volume":119159214.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-05T00:00:00+0000"},{"open":248.14,"high":261.86,"low":247.6,"close":261.16,"volume":129721567.0,"adj_high":261.86,"adj_low":247.6,"adj_close":261.16,"adj_open":248.14,"adj_volume":129721567.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-04T00:00:00+0000"},{"open":248.61,"high":250.02,"low":244.45,"close":246.53,"volume":101985305.0,"adj_high":250.02,"adj_low":244.45,"adj_close":246.53,"adj_open":248.61,"adj_volume":101985305.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-03T00:00:00+0000"},{"open":244.81,"high":254.2799,"low":242.62,"close":251.6,"volume":123810402.0,"adj_high":254.2799,"adj_low":242.62,"adj_close":251.6,"adj_open":244.81,"adj_volume":123810402.0,"split_factor":1.0,"dividend":0.0,"symbol":"TSLA","exchange":"XNAS","date":"2023-10-02T00:00:00+0000"}]}
2023-12-01 01:40:01,781 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:40:01,784 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:40:01,784 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:40:02,149 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:40:03,504 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:40:03,948 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 01:42:39,585 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:42:39,587 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:42:39,588 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:42:39,941 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:42:41,298 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:42:41,717 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 01:48:56,685 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:48:56,688 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:48:56,688 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:48:57,050 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:48:58,387 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:48:58,824 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 01:54:01,124 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:54:01,128 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:54:01,128 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:54:01,488 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:54:02,916 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:54:03,330 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 01:54:03,346 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 01:54:03,346 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 01:54:03,346 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 01:54:03,346 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 01:54:06,152 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 01:55:27,752 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:55:27,755 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:55:27,756 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:55:28,188 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:55:29,605 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:55:30,006 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 01:56:58,149 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 01:56:58,152 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 01:56:58,152 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 01:56:58,509 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 01:56:59,958 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 01:57:00,356 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 02:00:08,206 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 02:00:08,208 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 02:00:08,209 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 02:00:08,576 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 02:00:09,945 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 02:00:10,375 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 02:01:29,824 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 02:01:29,826 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 02:01:29,827 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 02:01:30,183 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 02:01:31,524 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 02:01:31,931 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 02:01:31,946 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 02:01:31,947 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 02:01:31,947 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 02:01:31,947 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 02:01:34,652 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 10:58:04,784 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 10:58:04,787 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 10:58:05,179 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at ExternalWorker.PSGWorker.<init>(PSGWorker.scala:15)
	at ExternalWorker.ExtFactory.get_consumer(ExtWorker.scala:37)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:13)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 20 common frames omitted
2023-12-01 10:58:05,195 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 10:58:07,208 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@6ddee60f.pgs_usr
2023-12-01 10:58:07,208 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 10:58:08,169 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 10:58:08,598 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 10:58:08,613 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 10:58:08,613 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 10:58:08,613 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 10:58:08,613 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 10:58:10,780 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 11:07:01,826 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:07:01,830 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:07:02,180 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at ExternalWorker.PSGWorker.<init>(PSGWorker.scala:15)
	at ExternalWorker.ExtFactory.get_consumer(ExtWorker.scala:37)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:13)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 20 common frames omitted
2023-12-01 11:07:02,196 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:07:03,827 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@1095d23a.pgs_usr
2023-12-01 11:07:03,827 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:07:04,472 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:07:04,869 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:07:04,885 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:07:04,885 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:07:04,885 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:07:04,885 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 11:07:06,995 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 11:09:00,204 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:09:00,204 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:09:00,570 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at ExternalWorker.PSGWorker.<init>(PSGWorker.scala:15)
	at ExternalWorker.ExtFactory.get_consumer(ExtWorker.scala:37)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:14)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 20 common frames omitted
2023-12-01 11:09:00,582 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:09:02,197 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@373e6b9d.pgs_usr
2023-12-01 11:09:02,197 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:09:02,857 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:09:03,222 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:09:03,238 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:09:03,238 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:09:03,238 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:09:03,238 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 11:09:05,871 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 11:15:49,294 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:15:49,299 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:15:49,633 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at ExternalWorker.PSGWorker.<init>(PSGWorker.scala:15)
	at ExternalWorker.ExtFactory.get_consumer(ExtWorker.scala:37)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:14)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 20 common frames omitted
2023-12-01 11:15:49,645 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:15:51,254 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@7945b206.pgs_usr
2023-12-01 11:15:51,254 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:15:51,950 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:15:52,381 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:15:52,398 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:15:52,398 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:15:52,398 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:15:52,398 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 11:15:55,005 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-01 11:43:25,438 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:43:25,440 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:43:25,800 WARN  [main] org.apache.hadoop.util.Shell - Did not find winutils.exe: {}
java.io.FileNotFoundException: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset. -see https://wiki.apache.org/hadoop/WindowsProblems
	at org.apache.hadoop.util.Shell.fileNotFoundException(Shell.java:548)
	at org.apache.hadoop.util.Shell.getHadoopHomeDir(Shell.java:569)
	at org.apache.hadoop.util.Shell.getQualifiedBin(Shell.java:592)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:689)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.conf.Configuration.getBoolean(Configuration.java:1691)
	at org.apache.hadoop.security.SecurityUtil.setConfigurationInternal(SecurityUtil.java:104)
	at org.apache.hadoop.security.SecurityUtil.<clinit>(SecurityUtil.java:88)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:312)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:300)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:575)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2510)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2510)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:314)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2690)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:949)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:943)
	at ExternalWorker.PSGWorker.<init>(PSGWorker.scala:15)
	at ExternalWorker.ExtFactory.get_consumer(ExtWorker.scala:37)
	at PersistStuct.Coordinator.<init>(Coordinator.scala:14)
	at MainApp$.main(MainApp.scala:12)
	at MainApp.main(MainApp.scala)
Caused by: java.io.FileNotFoundException: HADOOP_HOME and hadoop.home.dir are unset.
	at org.apache.hadoop.util.Shell.checkHadoopHomeInner(Shell.java:468)
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:439)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:516)
	... 20 common frames omitted
2023-12-01 11:43:25,810 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:43:27,426 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@6b1b19cf.pgs_usr
2023-12-01 11:43:27,427 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:43:28,106 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:43:28,569 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:43:28,581 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:43:28,584 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:43:28,584 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:43:28,584 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 11:46:06,110 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:46:06,110 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:46:06,473 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:46:08,033 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@51e3d37e.pgs_usr
2023-12-01 11:46:08,033 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:46:08,681 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:46:09,102 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:46:09,110 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:46:09,110 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:46:09,110 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:46:09,110 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-01 11:48:51,260 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-01 11:48:51,260 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-01 11:48:51,620 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-01 11:48:53,195 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@1da1380b.pgs_usr
2023-12-01 11:48:53,195 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-01 11:48:53,835 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-01 11:48:54,250 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-01 11:48:54,260 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-01 11:48:54,260 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-01 11:48:54,260 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-01 11:48:54,260 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 19:04:31,502 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 19:04:31,505 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 19:04:31,952 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 19:06:24,343 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 19:06:24,343 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 19:06:24,699 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 19:06:26,078 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@4e224df5.pgs_usr
2023-12-05 19:06:26,078 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 19:06:27,009 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 19:06:27,571 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 19:06:27,580 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 19:06:27,580 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 19:06:27,580 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 19:06:27,580 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 19:06:30,590 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 19:41:22,150 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 19:41:22,153 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 19:41:22,507 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 19:41:23,915 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 19:41:23,915 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 19:41:24,562 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 19:41:24,953 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 19:41:24,969 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 19:41:24,969 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 19:41:24,969 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 19:41:24,969 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 19:44:53,371 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 19:44:53,371 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 19:44:53,721 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 19:44:55,068 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 19:44:55,068 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 19:44:55,685 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 19:44:56,062 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 19:44:56,077 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 19:44:56,078 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 19:44:56,078 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 19:44:56,078 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 19:45:49,269 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 19:45:49,271 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 19:45:49,628 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 19:45:50,970 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 19:45:50,970 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 19:45:51,588 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 19:45:51,987 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 19:45:52,005 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 19:45:52,005 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 19:45:52,005 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 19:45:52,005 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 19:45:53,824 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 21:03:47,731 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:03:47,733 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:03:48,106 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:03:49,529 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 21:03:49,529 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:03:50,193 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:03:50,595 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:03:50,606 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:03:50,606 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:03:50,606 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:03:50,606 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:11:10,511 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:11:10,512 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:11:10,865 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:11:12,270 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 21:11:12,270 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:11:12,904 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:11:13,332 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:11:13,349 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:11:13,349 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:11:13,349 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:11:13,349 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:11:15,153 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 21:12:17,431 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:12:17,434 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:12:17,781 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:12:19,138 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 21:12:19,138 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:12:19,779 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:12:20,182 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:12:20,200 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:12:20,200 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:12:20,200 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:12:20,200 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:12:21,932 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 21:13:19,548 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:13:19,550 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:13:19,899 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:13:21,296 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 21:13:21,296 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:13:21,939 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:13:22,320 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:13:22,324 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:13:22,324 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:13:22,324 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:13:22,324 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:13:24,158 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 21:21:29,255 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:21:29,255 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:21:29,608 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:21:30,972 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-05 21:21:30,988 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:21:31,637 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:21:32,065 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:21:32,081 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:21:32,081 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:21:32,081 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:21:32,081 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:21:32,081 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-05 21:21:33,929 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully loaded
2023-12-05 21:42:47,578 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-05 21:42:47,581 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-05 21:42:47,927 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-05 21:42:49,277 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@781711b7.pgs_usr
2023-12-05 21:42:49,291 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-05 21:42:49,898 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-05 21:42:50,303 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-05 21:42:50,322 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-05 21:42:50,322 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-05 21:42:50,322 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-05 21:42:50,322 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-05 21:42:50,322 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-05 21:42:52,200 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 20:39:41,162 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 20:39:41,166 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 20:39:41,603 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 20:39:43,339 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-12 20:39:43,339 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 20:39:44,258 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 20:39:44,730 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 20:39:44,749 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 20:39:44,749 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 20:39:44,749 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 20:39:44,749 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 20:39:44,749 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 20:39:46,895 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 20:43:08,051 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 20:43:08,053 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 20:43:08,413 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 20:43:09,838 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@781711b7.pgs_usr
2023-12-12 20:43:09,839 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 20:43:10,492 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 20:43:10,884 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 20:43:10,903 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 20:43:10,903 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 20:43:10,903 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 20:43:10,903 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 20:43:10,903 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 20:43:12,839 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 20:43:13,117 INFO  [main] ExternalWorker.PSGLoader -  Create PSGWorker for t_source_marketdata table
2023-12-12 20:43:13,957 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 1 rows
2023-12-12 20:44:41,152 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 20:44:41,154 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 20:44:41,543 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 20:44:42,985 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-12 20:44:42,986 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 20:44:43,632 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 20:44:44,020 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 20:44:44,037 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 20:44:44,037 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 20:44:44,037 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 20:44:44,037 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 20:44:44,037 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 20:44:46,032 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 20:44:46,346 INFO  [main] ExternalWorker.PSGLoader -  Create PSGWorker for t_source_marketdata table
2023-12-12 20:44:47,110 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 1 rows
2023-12-12 20:53:04,259 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 20:53:04,261 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 20:53:04,620 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 20:53:06,074 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-12 20:53:06,074 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 20:53:06,747 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 20:53:07,152 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 20:53:07,172 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 20:53:07,172 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 20:53:07,172 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 20:53:07,172 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 20:53:07,172 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 20:53:09,071 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 21:06:25,645 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 21:06:25,647 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 21:06:26,020 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 21:06:27,494 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@642a16aa.pgs_usr
2023-12-12 21:06:27,494 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 21:06:28,145 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 21:06:28,567 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 21:06:28,584 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 21:06:28,586 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 21:06:28,586 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 21:06:28,586 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 21:06:28,586 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 21:06:30,446 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 21:06:30,565 INFO  [main] ExternalWorker.PSGLoader -  Create PSGWorker for t_source_marketdata table
2023-12-12 21:06:31,399 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows
2023-12-12 21:06:31,852 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-12 21:06:31,853 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-12 21:06:31,853 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-12 21:29:36,232 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 21:29:36,233 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 21:29:36,234 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-12 21:29:36,234 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 21:29:36,235 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 21:29:36,717 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 21:29:36,737 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 21:29:36,738 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 21:29:36,738 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 21:29:36,738 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 21:29:36,738 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 21:29:36,993 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 21:29:40,884 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 21:29:40,970 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-12 21:29:41,806 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to t_source_marketdata
2023-12-12 21:29:42,278 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-12 21:29:42,278 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-12 21:29:42,278 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-12 21:41:30,312 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 21:41:30,314 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 21:41:30,315 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-12 21:41:30,315 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 21:41:30,315 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 21:41:30,840 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 21:41:30,863 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 21:41:30,863 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 21:41:30,863 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 21:41:30,863 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 21:41:30,863 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 21:41:31,134 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 21:41:35,225 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 21:41:35,311 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-12 21:41:36,146 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to t_source_marketdata
2023-12-12 21:41:36,623 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-12 21:41:36,623 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-12 21:41:36,623 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-12 22:11:23,490 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-12 22:11:23,492 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-12 22:11:23,493 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-12 22:11:23,493 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-12 22:11:23,493 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-12 22:11:23,981 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-12 22:11:24,004 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-12 22:11:24,004 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-12 22:11:24,004 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-12 22:11:24,004 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-12 22:11:24,004 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-12 22:11:24,270 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-12 22:11:28,271 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-12 22:11:28,361 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-12 22:11:29,170 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to t_source_marketdata
2023-12-12 22:11:29,641 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-12 22:11:29,641 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-12 22:11:29,641 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-13 20:50:56,514 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:50:56,517 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:50:56,518 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:50:56,518 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:50:56,518 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:50:56,857 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 20:52:52,485 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:52:52,488 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:52:52,488 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:52:52,488 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:52:52,489 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:52:52,815 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 20:55:13,664 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:55:13,666 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:55:13,667 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:55:13,667 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:55:13,667 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:55:13,997 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 20:56:07,957 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:56:07,959 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:56:07,960 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:56:07,960 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:56:07,961 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:56:08,286 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 20:56:18,745 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:56:18,747 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:56:18,748 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:56:18,748 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:56:18,748 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:56:19,056 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 20:57:15,418 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 20:57:15,420 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 20:57:15,420 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 20:57:15,420 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 20:57:15,420 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 20:57:15,720 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 21:24:42,074 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 21:24:42,077 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 21:24:42,078 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 21:24:42,078 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 21:24:42,079 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 21:24:42,377 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-13 21:29:35,264 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-13 21:29:35,266 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-13 21:29:35,267 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-13 21:29:35,267 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-13 21:29:35,268 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-13 21:29:35,574 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 21:30:53,935 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 21:30:53,937 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 21:30:53,938 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 21:30:53,938 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 21:30:53,939 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-19 21:30:54,279 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 21:37:10,510 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 21:37:10,512 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 21:37:10,513 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 21:37:10,513 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 21:37:10,514 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-19 21:37:10,835 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 21:37:11,228 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 21:41:07,387 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 21:41:07,389 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 21:41:07,390 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 21:41:07,390 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 21:41:07,391 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-19 21:41:07,698 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 21:41:07,712 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDF: list size is 0
2023-12-19 21:41:08,020 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 21:41:48,506 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 21:41:48,508 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 21:41:48,510 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 21:41:48,510 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 21:41:48,510 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-19 21:41:48,806 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 21:41:48,819 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDF: list size is 16
2023-12-19 21:41:49,139 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 22:07:58,030 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 22:07:58,032 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 22:07:58,033 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 22:07:58,033 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 22:07:58,033 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for t_source_marketdata table
2023-12-19 22:07:58,316 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for t_source_marketdata table
2023-12-19 22:07:58,336 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDF: list size is 203
2023-12-19 22:07:58,662 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 22:20:19,286 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 22:20:19,289 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 22:20:19,289 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 22:20:19,289 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 22:20:19,289 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-19 22:20:19,608 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for m_data.t_source_marketdata table
2023-12-19 22:20:19,624 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDF: list size is 203
2023-12-19 22:20:19,926 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 22:21:15,856 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-19 22:21:15,858 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-19 22:21:15,858 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-19 22:21:15,858 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-19 22:21:15,858 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-19 22:21:16,408 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-19 22:21:16,429 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-19 22:21:16,429 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-19 22:21:16,429 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-19 22:21:16,429 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-19 22:21:16,429 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-19 22:21:16,693 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-19 22:21:20,750 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-19 22:21:20,846 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-19 22:21:21,854 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to m_data.t_source_marketdata
2023-12-19 22:21:22,221 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-19 22:21:22,221 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-19 22:21:22,221 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 19:13:09,515 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-21 19:13:09,519 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-21 19:13:09,519 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-21 19:13:09,519 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-21 19:13:09,519 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-21 19:13:10,017 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-21 19:13:10,037 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-21 19:13:10,037 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-21 19:13:10,037 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-21 19:13:10,037 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-21 19:13:10,037 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-21 19:13:10,389 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-21 19:13:14,977 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-21 19:13:15,064 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 19:13:16,171 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 0 rows to m_data.t_source_marketdata
2023-12-21 19:13:16,449 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-21 19:13:16,449 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-21 19:13:16,449 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 19:14:53,547 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-21 19:14:53,549 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-21 19:14:53,549 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-21 19:14:53,549 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-21 19:14:53,549 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-21 19:14:54,015 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-21 19:14:54,035 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-21 19:14:54,035 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-21 19:14:54,035 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-21 19:14:54,035 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-21 19:14:54,035 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-21 19:14:54,296 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-21 19:14:58,329 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-21 19:14:58,424 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 19:14:59,381 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to m_data.t_source_marketdata
2023-12-21 19:14:59,747 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-21 19:14:59,747 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-21 19:14:59,747 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 19:17:44,411 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-21 19:17:44,414 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-21 19:17:44,414 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@2e32ccc5.pgs_usr
2023-12-21 19:17:44,414 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-21 19:17:44,414 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-21 19:17:44,911 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-21 19:17:44,932 INFO  [main] WebApi.Request - Total amount of rows in request set to 4
2023-12-21 19:17:44,932 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-21 19:17:44,932 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-21 19:17:44,932 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-21 19:17:44,932 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-21 19:17:45,213 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-21 19:17:49,234 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-21 19:17:49,318 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 19:17:50,352 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 4 rows to m_data.t_source_marketdata
2023-12-21 19:17:50,719 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-21 19:17:50,719 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-21 19:17:50,719 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 19:33:54,810 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-21 19:33:54,812 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-21 19:33:54,813 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@78a773fd.pgs_usr
2023-12-21 19:33:54,813 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-21 19:33:54,813 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-21 19:33:55,315 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-21 19:33:55,337 INFO  [main] WebApi.Request - Total amount of rows in request set to 0
2023-12-21 19:33:55,338 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-21 19:33:55,338 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-21 19:33:55,338 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-21 19:33:55,338 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-21 19:33:55,595 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-21 19:33:59,288 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-21 19:33:59,360 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 19:34:00,367 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 0 rows to m_data.t_source_marketdata
2023-12-21 19:34:00,615 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-21 19:34:00,615 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-21 19:34:00,615 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 19:58:16,790 INFO  [main] ExternalWorker.WebApiWorker -  Create WebApiWorker for EOD datatype
2023-12-21 19:58:16,791 INFO  [main] ExternalWorker.ExtFactory -  Create producer for period stream (WEBAPI)
2023-12-21 19:58:16,792 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: connection for user PersistStuct.ConfigList@4f74980d.pgs_usr
2023-12-21 19:58:16,792 INFO  [main] ExternalWorker.ExtFactory -  Create consumer for period stream (POSTGRES)
2023-12-21 19:58:16,792 INFO  [main] ExternalWorker.WebApiWorker -  WebApiWorker: produce Struct Data
2023-12-21 19:58:17,296 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: respone ready
2023-12-21 19:58:17,318 INFO  [main] WebApi.Request - Total amount of rows in request set to 8
2023-12-21 19:58:17,321 INFO  [main] WebApi.Request - Page amount set to 1 rows
2023-12-21 19:58:17,321 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: parse is ready 
2023-12-21 19:58:17,321 INFO  [main] Streams.WebLoaderEOD - WebLoaderEOD: get_req_data DONE 
2023-12-21 19:58:17,321 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: loaded data is...
2023-12-21 19:58:17,570 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
2023-12-21 19:58:21,656 INFO  [main] PersistStuct.Coordinator - Coordinator.LoadData: successfully DONE
2023-12-21 19:58:21,756 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 19:58:22,707 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: start write 8 rows to m_data.t_source_marketdata
2023-12-21 19:58:23,067 INFO  [main] ExternalWorker.PSGLoader - PSGLoader: load complete
2023-12-21 19:58:23,067 INFO  [main] ExternalWorker.PSGWorker - PSGLoader: consume StructData complete
2023-12-21 19:58:23,067 INFO  [main] PersistStuct.Coordinator - Coordinator.SaveData: successfully DONE
2023-12-21 20:01:06,593 INFO  [main] ExternalWorker.PSGLoader - Create PSGWorker for null.t_source_marketdata table
2023-12-21 20:01:07,168 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDataStruct: start load for m_data.t_source_marketdata table
2023-12-21 20:01:07,192 INFO  [main] ExternalWorker.PSGLoader - PSGLoader.loadDF: list size is 223
2023-12-21 20:01:07,492 WARN  [main] org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
